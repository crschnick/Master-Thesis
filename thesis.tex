% !TeX spellcheck = en-US
% !TeX encoding = utf8
% !TeX program = lualatex
% !BIB program = biber
% -*- coding:utf-8 mod:LaTeX -*-


% vv  scroll down to line 200 for content  vv


\let\ifdeutsch\iffalse
\let\ifenglisch\iftrue
\input{pre-documentclass}
\documentclass[
  % fontsize=11pt is the standard
  a4paper,  % Standard format - only KOMAScript uses paper=a4 - https://tex.stackexchange.com/a/61044/9075
  twoside,  % we are optimizing for both screen and two-side printing. So the page numbers will jump, but the content is configured to stay in the middle (by using the geometry package)
  bibliography=totoc,
  %               idxtotoc,   %Index ins Inhaltsverzeichnis
  %               liststotoc, %List of X ins Inhaltsverzeichnis, mit liststotocnumbered werden die Abbildungsverzeichnisse nummeriert
  headsepline,
  cleardoublepage=empty,
  parskip=half,
  %               draft    % um zu sehen, wo noch nachgebessert werden muss - wichtig, da Bindungskorrektur mit drin
  draft=false
]{scrbook}
\input{config}


\usepackage[
  title={Transformed Sparse Grids for high-dimensional models},
  author={Christopher Schnick},
  type=master,
  institute=ipvs, % or other institute names - or just a plain string using {Demo\\Demo...}
  course={Informatik},
  examiner={Prof.\ Dr.\ Dirk Pflüger},
  supervisor={Dr. Michael Rehme},
  startdate={April 24, 2021},
  enddate={October 24, 2021}
]{scientific-thesis-cover}

\makeindex


\newtheorem{proof}{Proof}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\definecolor{charcoal}{HTML}{264653}
\definecolor{persian_green}{HTML}{2A9D8F}
\definecolor{light_cornflower_blue}{HTML}{8ECAE6}
\definecolor{eggshell}{HTML}{F4F1DE}
\definecolor{silver}{HTML}{C0C0C0}
\definecolor{persian_red}{HTML}{D03030}

\mdfdefinestyle{style}{%
linecolor=charcoal,
linewidth=1.3px,
innermargin=0,
innertopmargin=0,
frametitlebackgroundcolor=eggshell,
frametitlealignment=\center,
frametitlerulewidth=1.3px,
frametitlerule=true,
frametitlerulecolor=silver,
frametitleaboveskip=6.5pt,
frametitlebelowskip=3.5pt,
frametitlefont=\small\normalfont\bfseries
}

\mdfdefinestyle{algstyle}{%
linecolor=charcoal,
linewidth=1.3px,
innermargin=0,
innertopmargin=0,
frametitlebackgroundcolor=eggshell,
frametitlerulewidth=1.3px,
frametitlerule=true,
frametitlerulecolor=silver,
frametitleaboveskip=6.5pt,
frametitlebelowskip=3.5pt,
frametitlefont=\small\normalfont\bfseries
}


 \captionsetup[subfigure]{width=0.9\linewidth}


\newcommand{\Hsquare}{%
  \text{\fboxsep=-.2pt\fbox{\rule{0pt}{1.4ex}\rule{1.4ex}{0pt}}}%
}
\newcommand\red{red\,{\setlength\fboxsep{0pt}\colorbox{persian_red}{\Hsquare}} }
\newcommand\green{green\,{\setlength\fboxsep{0pt}\colorbox{persian_green}{\Hsquare}} }
\newcommand\grey{grey\,{\setlength\fboxsep{0pt}\colorbox{silver}{\Hsquare}} }
\newcommand\beige{beige\,{\setlength\fboxsep{0pt}\colorbox{eggshell}{\Hsquare}} }
\newcommand\lightblue{light blue\,{\setlength\fboxsep{0pt}\colorbox{light_cornflower_blue}{\Hsquare}} }
\newcommand\darkblue{dark blue\,{\setlength\fboxsep{0pt}\colorbox{charcoal}{\Hsquare}} }

\usepackage[normalem]{ulem}
\newcommand\coloruline{\bgroup\markoverwith{\textcolor{light_cornflower_blue}{\rule[-0.6ex]{2pt}{0.3pt}}}\ULon}
\crefformat{figure}{\coloruline{#2fig.$~$#1#3}}
\Crefformat{figure}{\coloruline{#2Fig.$~$#1#3}}
\crefformat{equation}{\coloruline{#2(#1#3)}}
\crefformat{section}{\coloruline{#2sec.$~$#1#3}}
\crefformat{chapter}{\coloruline{#2chap.$~$#1#3}}
\Crefformat{chapter}{\coloruline{#2Chap.$~$#1#3}}
\crefformat{definition}{\coloruline{#2def.$~$#1#3}}


\newcommand{\delimit}{{\color{charcoal}\noindent\rule{\textwidth}{1pt}}}

\begin{document}

%tex4ht-Konvertierung verschönern
\iftex4ht
  % tell tex4ht to create picures also for formulas starting with '$'
  % WARNING: a tex4ht run now takes forever!
  \Configure{$}{\PicMath}{\EndPicMath}{}
  %$ % <- syntax highlighting fix for emacs
  \Css{body {text-align:justify;}}

  %conversion of .pdf to .png
  \Configure{graphics*}
  {pdf}
  {\Needs{"convert \csname Gin@base\endcsname.pdf
      \csname Gin@base\endcsname.png"}%
    \Picture[pict]{\csname Gin@base\endcsname.png}%
  }
\fi

%\VerbatimFootnotes %verbatim text in Fußnoten erlauben. Geht normalerweise nicht.

\input{commands}
\setcounter{page}{1}
\pagenumbering{roman}
\Titelblatt

%Eigener Seitenstil fuer die Kurzfassung und das Inhaltsverzeichnis
\deftripstyle{preamble}{}{}{}{}{}{\pagemark}
%Doku zu deftripstyle: scrguide.pdf
\pagestyle{preamble}
\renewcommand*{\chapterpagestyle}{preamble}

%Kurzfassung / abstract
%auch im Stil vom Inhaltsverzeichnis
\ifdeutsch
  \section*{Kurzfassung}
\else
  \section*{Abstract}
\fi

Computer experiments provide a convenient framework to investigate real world phenomena, and making more information available to the simulation enables it to represent reality more accurately.
However, many simulation results mainly depend only on a subset of the inputs and are therefore possible targets for dimension reduction techniques, which aim to identify such inputs and allow to adapt the simulation accordingly, while preserving a required level of accuracy.\\
Various approaches to dimension reduction already exist in the context of sparse grids, such as adaptive sparse grids or sparse grid based analysis of variance, where the goal is to neglect or eliminate less important dimensions of model functions.
Active subspaces are another dimension reduction technique, which in contrast to sparse grid based methods, can replace parameters through linear combiniations of others, thus making them more flexible than purely dimension oriented approaches.\\
In this thesis, the active subspace method is applied in combination with sparse grids to obtain transformed sparse grids and create a more flexible dimension reduction technique that exploits the best of both worlds.
This combined approach views the model from a black-box perspective and therefore can and will be employed on a wide variety of high-dimensional real world models, with the goal to evaluate their general quality of the produced surrogates show their advantages and disadvantages in different contexts.

\cleardoublepage


% BEGIN: Verzeichnisse

\iftex4ht
\else
  \microtypesetup{protrusion=false}
\fi

%%%
% Literaturverzeichnis ins TOC mit aufnehmen, aber nur wenn nichts anderes mehr hilft!
% \addcontentsline{toc}{chapter}{Literaturverzeichnis}
%
% oder zB
%\addcontentsline{toc}{section}{Abkürzungsverzeichnis}
%
%%%

%Produce table of contents
%
%In case you have trouble with headings reaching into the page numbers, enable the following three lines.
%Hint by http://golatex.de/inhaltsverzeichnis-schreibt-ueber-rand-t3106.html
%
%\makeatletter
%\renewcommand{\@pnumwidth}{2em}
%\makeatother
%
\tableofcontents

\iftex4ht
\else
  %Optischen Randausgleich und Grauwertkorrektur wieder aktivieren
  \microtypesetup{protrusion=true}
\fi

% END: Verzeichnisse


% Headline and footline
\renewcommand*{\chapterpagestyle}{scrplain}
\pagestyle{scrheadings}
\pagestyle{scrheadings}
\ihead[]{}
\chead[]{}
\ohead[]{\headmark}
\cfoot[]{}
\ofoot[\usekomafont{pagenumber}\thepage]{\usekomafont{pagenumber}\thepage}
\ifoot[]{}


%% vv  scroll down for content  vv %%































%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Main content starts here
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Introduction}
\label{chap:c1}

\pagenumbering{arabic}
\setcounter{page}{1}

Sparse grids, a discretimzation scheme for functions on hypercubes, were introduced as an alternative to the more simple full grid discretisization scheme, which suffers heavily from the curse of dimensionality as the dimension of the functions grow, the amount of grid points required to generate a full grid grows exponentially.
As a result, the required model function evaluations also grow exponentially.
While all grid-based methods all share the common problem of being affected by the curse of dimensionality, sparse grids manage to reduce the impact of it and therefore delay the point where they become infeasible to use.
In this thesis the focus lies on high dimensional functions with $d > 8$ that cause even normal sparse grids to run into problems, since they are affected by the curse of dimensionality when dimensions reach that size.
These problems are caused by the high amount of grid points and result in too many required model function evaluations to first construct a sparse grid and also too high runtime and memory requirements for many algorithms that work on sparse grids.

Furthermore, since all grid points and tensor product basis functions used for sparse grids, are effectively axis-aligned, sparse grids, and therefore all techniques based on them, become rather ineffective when dealing with model functions that primarily consist out of function terms that are not axis-aligned.
This limitation is used as a motivation to look into ways into aligning the model function inputs, by first conducting a parameter study to identify potential starting points for a dimension reduction.
A parameter study is the process of exploring the influence of different function parameters on the function output.
This gained knowlegde about the parameters can be used for dimension reduction, which refers to the elimination and replacement of certain function parameters.
The insights gained from studying the inputs can then be used to transform and reduce the dimension of the model inputs prior to feeding them into sparse grids, s.t. the resulting sparse grid surrogates perform better wrt. the amount of grid points allocated and also the approximation error. 

Lastly, inspired from the concept of intrinsic dimension in signal processing and common dimension reduction methods in data science such as PCA, we try to combine the process of determining the optimal model input alignment along active directions with dimension reduction.
Done well, this allows us to transform the model function approximation problem into a lower-dimensional one, while still achieving an acceptable additional approximation error.
Overall, this enables us to apply sparse grids to much higher-dimensional problems as before.

\begin{mdframed}[style=style]

\vspace{2.5mm}
\begin{figure}[H]

\centering
\begin{tikzpicture}
\node[] at (9.2,0) {$d$};
\draw[->,ultra thick,charcoal] (0,0)--(9,0);
\foreach \x/\xtext in {0/$1$,1/$2$,2/$3$,3/$4$,4/$5$,5/$6$,6/$7$,7/$8$,8/$9$}
    \draw(\x,5pt)--(\x,-5pt) node[below] {\xtext};
    
\draw[decorate, decoration={brace}, thick, yshift=6.8ex]  (0,0) -- node[above=0.4ex] {full grids}  (3,0);
\draw[decorate, decoration={brace, mirror}, thick, yshift=2.5ex]  (7,0) -- node[above=0.4ex] {sparse grids}  (0,0);
\draw[decorate, decoration={brace, mirror}, thick, yshift=-5ex]  (0,0) -- node[below=0.6ex] {transformed sparse grids}
 (10,0);
\filldraw[white,thick] (9.9,-0.7) rectangle (11,-1.1);
 \draw[->,thick] (9.8,-0.91)--(11,-0.91);
\end{tikzpicture}

\vspace{2.5mm}
\delimit
\captionof{figure}{Approximate suitable input dimensions for different grid based techniques for model approximations.}
\label{fig:dim_ranges}
\end{figure}
\end{mdframed}

\section{Structure}

\Cref{chap:c2} covers all the basics needed to understand and use sparse grids in conjunction with different kinds of basis functions, spatial adaptivity, function interpolation and regression.
It will also give an overview over the currently most commonly used methods for dimension reduction with sparse grids..
The concept of input transformations, which are the central focus of this thesis, are introduced in \cref{chap:c3}.
Various types of transformations are discussed, but the main focus lies on linear transformations generated with the help of active subspaces.
In \cref{chap:c4} we look at the process of generating reduced sparse grid surrogates for a given input transformation to obtain transformed sparse grids.
\Cref{chap:c5} combines the transformation process of the previous chapters with an iterative approach, inspired from projection pursuit regression, and formalizes the theoretical transformation algorithms.
Then, in \cref{chap:c6}, we will cover the general structure of the practical implementation and explore various configuration options that can be used to fine tune different aspects of the surrogate creation process.
The implementation is then put to the test in \cref{chap:c7}, which will apply the new transformation-based approach to different models and compares the results with the conventional sparse grids.
The obtained evaluation results are used to draw conclusions about the technique in general and about its comparative quality.
The goal of \cref{chap:c8} is to deduce a decision process to determine optimal configuration options and to assign good default values to them, such that the method is easier to use.


\chapter{Surrogate construction}
\label{chap:c2}

Computer simulations almost always work with and on mathematical models and on the lowest level rely on evaluations of the underlying mathematical model.
More formally, we assume that this mathematical model is represented with a model function and denote the model function by $f \colon \Omega \to \mathds{R}$ where $\Omega \coloneqq [0,1]^d$ is the model domain, here a $d$-dimensional unit hypercube.
While the domain of many models may not be the unit hypercube, the domain can easily be translated and scaled to obtain the unit hypercube, as long as it is bounded.
Furthermore, many models are subject to uncertanties in their inputs.
To incorporate these uncertanties, we model the inputs stochastically with a probability distribution defined by a probability density function $\rho \colon \Omega \to \mathds{R_+}$.

Model function surrogates come into play in cases where it is expensive to directly evaluate $f$ as many operations on model functions, such as optimization, quadrature, and others, require many evaluations.
This is usually the case when a differential equation has to be solved by the model function.
The usual course of action is then to construct a surrogate $\hat{f}$ that has the advantage of being much faster to evaluate while preserving an acceptable amount of accuracy such that $\forall x \in \Omega \colon f(x)=\hat{f}(x)$ holds.
Moreover, the surrogate can come with other desirable properties, such as the gradients or moments being easily to calculate, and therefore providing additional useful features.

Plenty of different surrogate types and construction methods exist where each one comes with its own idiosyncrasies.
These surrogate type and construction method specific properties can range from their sensitivity to the input dimension of the model to their approximation quality in general or with regards to a specific type of model. 
The primary focus of this thesis lies on the already established sparse grid surrogates \cite{} and their various subtypes and construction methods, with the goal of enhancing the standard construction approach by finding and exploiting lower-dimensional important subsets of the model inputs.

\section{Full grids}

Model functions can easily be discretisized using a naive full grid approach, which involves creating a uniform grid that spans $\Omega$ and interpolates function values using neighbouring grid points.
More formally, we construct a one-dimensional mesh of level $\ell$ with mesh size $h=2^{-\ell}$ and use a basic tensor product approach to obtain a $d$-dimensional grid mesh.
By then using nodal basis functions at each grid point, we can interpolate functions very accurately.
The asymptotic interpolation error for full grids is $\mathcal{O}(h^2)$, however this comes at the cost of requiring $\mathcal{O}(2^{\ell d})$ grid points.
Full grids suffer heavily from the curse of dimensionality as the required amount of function evaluations for the grid points grows exponentially for $\ell$ and $d$.

\begin{definition}[Levels and indices]
Let
\begin{equation}
\underline{\ell} = (\ell_1, \dots, \ell_d) \in \mathds{N}_0^d
\end{equation}
be a $d$-dimensional multi level index.\\
For every multi level index $\underline{l}$, we define the hierarchical index set
\begin{equation}
H_{\underline{\ell}} \coloneqq \left\{ \underline{i} \in \mathbb{N}^d_0 \mid
\begin{cases}
    i_t=1,3\dots,2^{\ell_t} - 3, 2^{\ell_t} - 1, & \ell_t \geq 1 \\
    i_t=0,1, & \ell_t = 0
\end{cases} \right\}.
\end{equation}
\end{definition}

We use hierarchical index sets in order to define a full grid as the sum of grid increments that are constructed for a given multi level index $\underline{\ell}$ and its associated index set $H_{\underline{l}}$, such that every point in the grid mesh is assigned to one level and hierarchical index.

\begin{definition}[Grid points]
Let $x_{l,i}=i2^{-\ell}$ be a one-dimensional grid point coordinate, and let
\begin{equation}
x_{\underline{\ell},\underline{i}}=(x_{\ell_1,i_1}, \dots, x_{\ell_d,i_d})
\end{equation}
be a $d$-dimensional grid point.\\
The set of grid points of a regular full grid for a given level $\ell$ is then given by
\begin{equation}
X^{\text{f}}_{\ell}=\{x_{\underline{\ell'},\underline{i}} \mid |\underline{\ell'}|_{\infty} \leq \ell, \underline{i} \in H_{\underline{\ell}}\}.
\end{equation}
\end{definition}

This incremental construction will make it easier to transition from full grids to sparse grids later on.
Next, we assign each grid point a supporting basis function, where the basis functions usually stem from one family of basis function, like a family of linear or polynomial basis functions.

\begin{definition}[Basis functions]
Let $\phi_{\ell,i}(x)$ be univariate basis functions for the one-dimensional grid points $x_{\ell,i}$.\\
The multivariate $d-$dimensional basis function at a grid point $x_{\underline{\ell},\underline{i}}$ is then obtained by applying the tensor-product approach to get
\begin{equation}
\phi_{\underline{\ell},\underline{i}} \coloneqq \prod_{k=1}^{d} \phi_{\ell_k,i_k}.
\label{eq:basis_functions}
\end{equation}
\end{definition}

\begin{definition}[Full grid]
Let $\phi_{\underline{\ell},\underline{i}}$ be multivariate basis functions and $X$ a set of grid points.\\
The grid is the span of the basis functions over all grid points
\begin{equation}
V_{X}=\text{span}~ \{\phi_{\underline{\ell},\underline{i}} \mid x_{\underline{\ell},\underline{i}} \in X\}.
\end{equation}
For a set of full grid points $X^{\text{f}}_{\ell}$, the approximation space
\begin{equation}
V^{\text{f}}_{\ell}=\text{span}~ \{\phi_{\underline{\ell},\underline{i}} \mid x_{\underline{\ell},\underline{i}} \in X^{\text{f}}_{\ell}\}
\label{eq:full_grid}
\end{equation}
is referred to as a full grid of level $\ell$.

\end{definition}


\section{Sparse grids}

The span $V^{\text{f}}_{\ell}$ of a full grid can be decomposed into a direct sum of subspaces in a hierarchical way, to allow for a more granular selection of which subspaces to keep.
Keeping only a subset of subspaces results in a worse interpolation error compared to full grids with asymptotic error including a logarithmic factor with $\mathcal{O}(h^2 (\log h^{-1})^{d-1})$.
The amount of grid points however only grows with $\mathcal{O}(h^{-1} (\log h^{-1})^{d-1})$, which is a significant improvement over full grids and makes it possible to apply the sparse grid technique to higher dimensional problems than full grids.

\begin{definition}[Sparse grid points]
Let $\ell \in $ be a level.\\
Using the hierarchical index sets $H_{\underline{\ell}}$, we define the set of grid points for a regular sparse grid of level by
\begin{equation}
X^{\text{s}}_{\ell}=\{x_{\underline{\ell'},\underline{i}} \mid |\underline{\ell'}|_1 \leq \ell, \underline{i} \in H_{\underline{\ell}}\}.
\end{equation}
\end{definition}

\begin{definition}[Sparse grid]
Let $\phi_{\underline{\ell},\underline{i}}$ be multivariate basis functions and $X^{\text{s}}_{\ell}$ a set of sparse grid points.\\
The span of the basis functions
\begin{equation}
V^{\text{s}}_{\ell}=\text{span}~ \{\phi_{\underline{\ell},\underline{i}} \mid x_{\underline{\ell},\underline{i}} \in X^{\text{s}}_{\ell}\},
\label{eq:sparse_grid}
\end{equation}
also called the approximation space, is referred to as a sparse grid of level $\ell$.
\end{definition}

A visualization of the hierarchical splitting of the grid point set and the construction of sparse grids and full grids can be seen in \cref{fig:grid_construction}.

\begin{mdframed}[style=style]
\vspace{2.5mm}
\begin{figure}[H]
        \centering
\begin{minipage}{.73\textwidth}
        \centering
  \includegraphics[width=.8\linewidth]{graphics/grid_subspaces}
  \subcaption{Hierarchical grid point splitting}
    \end{minipage}%
    \begin{minipage}{0.26\textwidth}
        \centering
  \includegraphics[width=.8\linewidth]{graphics/full_grid}
  \subcaption{Grid points of $V^{\text{f}}_{\ell}$}
\vspace{4.5mm}
  \includegraphics[width=.8\linewidth]{graphics/sparse_grid}
  \subcaption{Grid points of $V^{\text{s}}_{\ell}$}
    \end{minipage}
    
\vspace{2.5mm}
\delimit

\captionof{figure}{Grid points of generated spatially adaptive sparse grids for $f_1$ and $f_2$ that started out as a regular sparse grid with level $l=1$. It used the surplus refinement criterion, 15 refinements steps with one grid point refinement per step, and modified linear basis functions.}
\label{fig:grid_construction}
\end{figure}
\end{mdframed}

In related sparse grid literature, the hierarchical decomposition of sparse grids of \cref{eq:sparse_grid}, which can be seen in \cref{fig:grid_construction}, is realized through the decomposition of subspaces instead of decomposition of the grid point set.
This allows for easier proofs of the approximation qualities for sparse grids, but is not required here.

\begin{definition}[Hierarchical subspaces]
The span of the basis functions $\phi_{\underline{\ell},\underline{i}}$ for of a grid increment indentified by a multi level index $\underline{\ell}$,
\begin{equation}
W_{\underline{\ell}}=\text{span}~ \{\phi_{\underline{\ell'},\underline{i}} \mid \underline{i} \in H_{\underline{\ell}}\},
\end{equation}
is defined as the hierarchical subspace of $\underline{\ell}$.\\
A sparse grid can then also be represented as a direct sum of hierarchical subspaces:
\begin{equation}
V^{\text{s}}_{\ell}=\text{span}~ \{\phi_{\underline{\ell},\underline{i}} \mid x_{\underline{\ell},\underline{i}} \in X^{\text{s}}_{\ell}\}=\bigoplus_{|\underline{\ell'}|_1 \leq \ell} W_{\underline{\ell'}}
\label{eq:direct_sum}
\end{equation}
\end{definition}

The sparse grid construction with \cref{eq:sparse_grid} is equal to \cref{eq:direct_sum}, as the only difference between both is that the same hierarchical decomposition is realized in different stages of the sparse grid construction.


\section{Basis functions}

The multivariate basis functions $\phi_{\underline{\ell},\underline{i}}$, as defined in \cref{eq:basis_functions}, are a central aspect of sparse grids.
Many different basis function families exist, each with various levels of complexity, capabilities, and properties
In this section we will take a look at all the different basis functions used in this thesis and the reasons for using them.

\begin{description}
\item[Hat functions] A one-dimensional hat function is defined by $h(x)=\max(1 - |x|,0)$. They can be transformed into basis functions for a level $l$ and index $i$ with $\phi^h_{l,i}(x) \coloneqq h(2^lx-i)$. It has the value of $1$ at their grid support point $\phi^h_{l,i}(x_{l,i})=1$ and $0$ at their neighbouring grid points $\phi^h_{l,i}(x_{l,i-1})=\phi^h_{l,i}(x_{l,i-1})=0$. They are not differentiable at their grid point.

\item[Prewavelets] A wavelet is a function that has wavelike properties usually with some kind of oscillation around zero.
Many types of wavelet functions are used for signal processing applications because they inhibit some advantageous properties.
One of these properties is the orthogonality property with $\langle \phi_{i},\phi_{j} \rangle = 0$ for all wavelets with $i \neq j$.
Furthermore,  the wavelets are usually discretisized, because it is not possible to analyze a signal using an infinite amount of wavelets.
One wavelet basis that is already used with sparse grids is the so-called mexican hat basis presented in \cite{}, which does not fullfil any orthogonality property.
In fact, using a completely orthogonal basis is quite constraining, i.e. it is not easy to construct a usable orthogonal basis for the context of sparse grids.
However, in the context of sparse grid based ANOVA we can also work with a semi-orthogonal wavelet basis, also called a prewavelet basis.

\item[Modified basis] One method of removing the need of using boundary grid points at least to some degree are modified basis functions.
These basis functions do not require grid points at the boundary.
Instead at every grid level, they extrapolate the values at the boundaries from the grid points closest to the boundaries using a special kind of basis function.
Therefore, it is possible to modify any non-boundary basis $\phi_{l,i}$ by defining the modified basis as follows:
\begin{equation}
\phi^{\text{mod}}_{l,i}(x) \coloneqq
\begin{cases}
1 &, l=1\\
\phi^{\text{left}}_{l}(x)&, l>1, i=1\\
\phi^{\text{right}}_{l}(x)&, l>1, i=2^l - 1\\
\phi_{l,i}(x)&, \text{else}
\end{cases}
\nonumber
\end{equation}
where $\phi^{\text{left}}_{l}$ and $\phi^{\text{right}}_{l}$ are the special boundary extrapolation functions.
Usually $\phi^{\text{left}}_{l}$ and $\phi^{\text{right}}_{l}$ are similar with regards to their structure of the normal basis functions $\phi_{l,i}$, e.g. a hat basis is usually modified with special hat functions.

\item[Splines] One solution to eliminating the hat basis discontinuities are B-Splines, which are just piecewise polynomials.
They can also be combined with the modified basis technique to obtain a modified B-Spline basis.
\end{description}

\todo{Image}

\section{Grid surrogate construction}

The are multiple different approaches of constructing a sparse grid surrogate to approximate a model function.
A sparse grid surrogate $\hat{f}$ is a linear combination of all basis functions as $\hat{f} \in V^{\text{s}}_{\ell}$.
Therefore, to represent $\hat{f}$, each basis function is assigned a weight, also called hierarchical surplus, $\alpha_{\underline{\ell},\underline{i}}$.

\begin{definition}[Sparse grid surrogates]
Let $\phi_{\underline{\ell},\underline{i}}$ be multivariate basis functions and $X$ a set of grid points.
Furthermore, let $\alpha_{\underline{l},\underline{i}} \in \mathds{R}$ be the hierarchical surplusses.
A sparse grid surrogate is then
\begin{equation}
\hat{f}(x) \coloneqq \sum_{x_{\underline{\ell},\underline{i}} \in X} \alpha_{\underline{\ell},\underline{i}} ~ \phi_{
\underline{\ell},\underline{i}}(x)
\end{equation}
In the case of a regular sparse grid, i.e. $X=X^{\text{s}}_{\ell}$, the surrogate simply is
\begin{equation}
\hat{f}(x) \coloneqq \sum_{|\underline{\ell}|_1 \leq \ell} \sum_{\underline{i} \in {H_{\underline{\ell}}}} \alpha_{\underline{\ell},\underline{i}} ~ \phi_{
\underline{\ell},\underline{i}}(x).
\end{equation}
\end{definition}

The interpolation method is the most simple and common sparse grid surrogate construction scheme.
It evaluates the model function at every grid point and performs a hierarchisation algorithm \cite{} to obtain the hierarchical surplusses, $\alpha_{\underline{l},\underline{i}}$.
The resulting surrogate constructred with the hierarchisation method equals the model function at all grid points, i.e. $\forall x_{\underline{l},\underline{i}} \in X \colon f(x)=\hat{f}_{\text{hier}}(x)$.
Furthermore, the method requires the model function to be evaluated at every grid point, which means that it requires exactly $|X|$ function evaluations and it can not be used to approximate a function from a given function sample.

Another method of constructing the surrogate is by using a regression approach \cite{}.
In contrast to interpolation, we no longer require the function values at the grid points to interpolate between them.
Instead, the regression method uses established regression approaches to iteratively improve the $\alpha_{\underline{l},\underline{i}}$. wrt. to the approximation error with the help of train and validation samples taken from the model function and input distribution.
As a result, the size of the function sample and therefore function evaluations, does no longer have to be equal to the amount of grid points.
Therefore there is a greater level of flexibility as the amount of model function evaluations and the amount of grid points can vary.
The basic approach is to solve the least squares problem for $n$ input samples $x_1, \dots, x_n$ with the standard square loss function
\begin{equation}
\epsilon(\hat{f})=\frac{1}{n} \sum_{i=1}^n (f(x_i) - \hat{f}(x_i))^2 
\end{equation}
and a regularization parameter $\lambda > 0$ to prevent overfitting.
I.e. sparse grid based regression tries to solve the regularized least squares problem to find the best possible surrogate for a set of possible smoothing factors $\Lambda$ and sparse grid level $\ell$ with
\begin{equation}
\hat{f} = \argmin_{\hat{f} \in V^{\text{s}}_{\ell}, ~ \lambda \in \Lambda} \epsilon(\hat{f}) + \lambda R(\hat{f}).
\end{equation}

\section{Spatially adaptive sparse grids}

In many cases, generating a regular sparse grid can be improved upon by introducing adaptivity into the process.
This allows us to spent more grid points, and therefore be more accurate, in regions that are more important and conversely reduce the amount of grid points is less important regions.
One method of realizing adaptivity in sparse grids are spatially adaptive sparse grids \cite{}.
By defining a grid point hierarchy such that each non-boundary grid point has two child grid points in every dimension, one can create a stepwise algorithm that creates these child grid points if they are needed and do not exist yet.
This creation of new grid points of a parent is called refinement of a grid point.

\begin{definition}[Grid point children]
Let $x_{\underline{\ell},\underline{i}} \in X$ be a grid point.
We then define
\begin{equation}
c_{k}^{\text{left}}(x_{\underline{\ell},\underline{i}})=(x_{\ell_1,i_1}, \dots, x_{\ell_k + 1,2  i_k - 1}, \dots, x_{\ell_d,i_d}), ~~ 
\end{equation}
as its left child if $i_k > 0$ and
\begin{equation}
c_{k}^{\text{right}}(x_{\underline{\ell},\underline{i}})=(x_{\ell_1,i_1}, \dots, x_{\ell_k + 1,2  i_k + 1}, \dots, x_{\ell_d,i_d}), ~~ 
\end{equation}
as its right child if $i_k < 2^{\ell_k}$.
Then
\begin{equation}
c_{k}(x_{\underline{\ell},\underline{i}})=
\begin{cases}
\{c_{k}^{\text{right}}(x_{\underline{\ell},\underline{i}})\}&, i_k=0\\
\{c_{k}^{\text{left}}(x_{\underline{\ell},\underline{i}})\}&,i_k= 2^{l_k}\\
\{c_{k}^{\text{left}}(x_{\underline{\ell},\underline{i}}),c_{k}^{\text{right}}(x_{\underline{\ell},\underline{i}}) \}&, \text{else}
\end{cases}
\end{equation}
is the set of possible children of a grid point in dimension $k$ and 
\begin{equation}
c(x_{\underline{\ell},\underline{i}})= \bigcup_{k=1}^d \{c_{k}(x_{\underline{\ell},\underline{i}})\}
\end{equation}
is the set of children of a grid point. It holds that $|c(x_{\underline{\ell},\underline{i}})| \leq 2d$ and $|c(x_{\underline{\ell},\underline{i}})| =2d$ if there is no boundary involved.
We define a grid point $x_{\underline{\ell},\underline{i}} \in X$ as a leaf if
\begin{equation}
c(x_{\underline{\ell},\underline{i}}) \cap X = \emptyset
\end{equation}
\end{definition}

To choose the best grid points to refine, a refinement criterion is introduced.
This abstract refinement criterion assigns a value to grid point and $r$ grid points with the highest value are refined.

\begin{definition}[Refinement criterion]
Let $X$ be a set of grid points.
Then $\xi \colon X \mapsto \mathds{R}$ denotes the refinement criterion function, which assins each grid point a value that determines the suitability of a refinement of that grid point.

Let $\hat{f} \in V^X$.
The commonly used surplus criterion is defined as
\begin{equation}
\xi_s(x_{\underline{\ell},\underline{i}}) = |\alpha_{\underline{\ell},\underline{i}}|
\end{equation}
Since in this thesis we also assume that the input is distributed according to the probability density $\rho$, we also introduce the distribution surplus criterion with
\begin{equation}
\xi_{ds}(x_{\underline{\ell},\underline{i}}) =\rho(x_{\underline{\ell},\underline{i}}) |\alpha_{\underline{\ell},\underline{i}}|
\end{equation}
\end{definition}
We define the grid points a of adaptive sparse grids after $m$ steps as $X_a^{(m)}$
A spatially adaptive sparse grid usually starts out as a regular sparse grid with a coarse level, i.e. $X_a^{(0)}=X_{\underline{\ell}}$.
Then during the $m$-th iteration, we first order all leaf grid points $x_i$ with $x_1, \dots, x_p \in X_a^{(k-1)}, ~~ \xi(x_1) \geq \dots \geq \xi(x_p)$ and then refine with $X_a^{(m)}=X_a^{(m-1)} \cup c(x_1) \cup \dots \cup c(x_r)$.
Usually, if a leaf grid point is refined, all children are created because selective grid point creation would create additional overhead.
After $n$ steps, the spatially adaptive sparse grid with the grid points $X_a^{(n)}$ is finished.
The important parameters that influence the results are the iterations $n$, the refinements per step $r$, the refinement criterion $\xi$ and the initial grid type and size.

The concept of spatially adaptive Sparse Grids is similar to other established adaptivity methods, such as the preceededing dimensionally adaptive sparse grids \cite{} or the more recent spatially dimension adaptive Sparse Grids \cite{}.
One downside of the classical version of spatially adaptive sparse grids is that all children grid points are added in a refinement step.
In a dimension reduction focused setting this behaviour is not optimal, since we want to focus on more important dimensions only when refining a grid point.
Spatially dimension adaptive Sparse Grids do exactly that, and are therefore better suited for dimension reduction.

\section{Sparse Grid based ANOVA}

Functional Analysis of Variance (ANOVA)  is a well established method in statistics to identify important inputs of a model.
It has also already been thoroughly investigated for this purpose in the context of sparse grids \cite{F10}.
It is based on variance-based sensitivity analysis, a form of global sensitivity analysis introduced by Sobol \cite{S01}, which determines the importance of individual model inputs.
Thus it allows to neglect less important ones.
In contrast to the previously described methods, Sparse Grid based ANOVA is purposefully designed for dimension reduction.
The fundemental goal is to obtain the ANOVA decomposition of a function $f$ that has the form of
\begin{equation}
f(x)=f_0 + \sum_{t=1}^d \sum_{i_1 < \dots < i_t} f_{i_1,\dots,i_t}(x_{i_1},\dots,x_{i_t})
\label{anovaBase}
\end{equation}
where all terms have to be orthogonal.
Let $D=\{1,\dots,d\}$ be the set of dimension indices.
Then $C \coloneqq \mathcal{P}(D)$ is the set of ANOVA-components, where an ANOVA-component $c \in C$ that contains a specific dimension index $i \in D$ indicates that the function term is non-constant in the $i$-th dimension.
We can then express the decomposition \eqref{anovaBase} as a componentwise sum with
\begin{equation}
f(x)=\sum_{c \in \mathcal{P}(D)} f_{c}(x)
\label{anovaComp}
\end{equation}
As mentioned before, a requirement of this decomposition is that all terms have to be orthogonal, i.e. for $c,c' \in C, c \neq c'$ it has to hold that
\begin{equation}
\int_{\Omega} f_c(x) f_{c'}(x) \text{d}x = 0
\end{equation}
It can be shown that this property can be fullfilled by having terms that average to zero over all of their active dimension indices with
\begin{equation}
\int_0^1 f_{i_1,\dots,i_t}(x_{i_1},\dots,x_{i_t}) ~ \text{d}x_k = 0, ~ k \in \{i_1, \dots, i_t\}
\end{equation}



A decomposition of a high dimensional function $f$ into $2^d$ lower dimensional function terms allows for an examination of individual terms with the goal identifying the terms that contribute the least amount of variance to the total function variance.
This decomposition can be employed on sparse grids by using an orthogonal basis, e.g. a wavelet basis, with a constant basis function to represent the $f_c$.

Since the terms $f_i$ fullfil the orthogonality property \eqref{ortho}, we can define a variance function $\sigma^2(f)$ in an additive manner:
\begin{equation}
\sigma^2(f) \coloneqq \int_{\Omega} f(x)^2 \rho(x) ~ \text{d} x=\sum_{c \in \mathcal{P}(D)} \sigma^2(f_c)
\end{equation}
For every ANOVA component $c \in C$, we can then calculate the variance share, also called Sobol index, for each term:
\begin{equation}
S_{c}=\frac{\sigma^2(f_{\underline{c}} )}{\sigma^2(f)}, ~~ \sum_{c \in \mathcal{P}(D)} S_{c} = 1
\nonumber
\end{equation}

This can then be used for effective dimensionality reduction as shown in \cite{G13,F10}, where the focus lies on minimizing the ANOVA order $|c|$, i.e. the number of non-constant dimensions of the function terms used in \eqref{anovaBase}, to approximate $f(x)$.
In other words, we try to find a $u < d$ with
\begin{equation}
f(x) \approx \sum_{c \in \mathcal{P}(D), |c| \leq u} f_c(x)
\label{effectiveAnova}
\end{equation}
Such an approximation is useful for certain problems with a low effective dimensionality as presented in \cite{H08}.
This method also has its limitations however, specifically the need to first construct a $d$-dimensional sparse grid to accurately compute the variances $\sigma^2(f_c)$, the need for boundaries and the analysis of $2^d$ terms.

Alternatively, the variances of the function terms can also be calculated with a Monte-Carlo approach \cite{} beforehand and the grid can be generated accordingly, removing the need to first generate all functions term in \ref{anovaComp} and then cutting some of them off again in \ref{effectiveAnova}.
However, one way or the other, one has to deal with the negative effects of the curse of dimensionality, since all calculations to determine variances are done only on the original high-dimensional model.


\section{Radial basis functions}

A radial basis function $\varphi \colon \Omega \mapsto \mathds{R}$ is a function, whose value solely depends on the distance between its input and some center point $c \in \Omega$, i.e. it holds that $\varphi(x)=\varphi(\|x - c\|)$.
They can also be used to for function approximation with 
\begin{equation}
f(x) \approx \sum_{i=1}^n \omega_i \varphi(\|x - c_i\|)
\end{equation}
In the context of this thesis, only the euclidean distance is used as a distance measure and Gaussian functions \cite{} are used as basis functions.
They take the following form:
\begin{equation}
\varphi(x)=e^{-(\epsilon \|x - c_i\|)^2}
\end{equation}
The only parameter that can be tuned is $\epsilon$, which is referred to as the shape parameter and is used to scale the effects of the input radius.

Given a set of center points, type of basis functions, and shape parameter $\epsilon$, we can calculate the weight coefficients $\omega_i$.
There are several ways of achieving this.
It is usually done using linear least squares technique and involves solving an $m \times m$-dimensional system of linear equations.
This, combined with the fact that exponentially more center points are needed to maintain the same level accuracy when model dimensions increase, also causes this method to suffer from the curse of dimensionality.

The reason radial basis functions are mentioned here is because they are almost a polar opposite to the grid based approximation methods.
Radial basis function surrogates allow arbitrary center points, while Sparse Grid surrogates require a strict, hierarchical, and axis-aligned grid points.
Furthermore, radial basis functions are fundamentally different to tensor-product based basis functions wrt. to alignment.
They are not alignment-based, i.e. given a transformation $t(x)$, the approximation quality stays the same when transformation the center points and function inputs, while it changes for tensor-product basis function.



\section{Sampling}

We assume that the inputs of the model functions are distributed according to multivariate distribution $\rho(x)=\prod_{i=1}^d \rho_i(x_i)$, which consists out of $d$ independent distributions $\rho_i$.
In the coming chapters we will use some Monte-Carlo based methods as a comparision to the presented sparse grid methods.
For this purpose, even the relatively simple process of sampling a probality distribution can improved in different ways to create better results.

\subsection{Quasirandom sequences}

A problem that arises when generating a sequence of random samples using a pseudorandom number generator to draw samples from the input distribution $\rho$ is that the sequence does not cover the function domain as even and quick as it theoretically can.
One solution are low-discrepancy sequences, also called quasirandom sequences to distinguish them from pseudorandom sequencens.
These sequences are more evenly distributed than pseudorandom samples and therefore have a lower discrepancy.
As a result they cause the result of a Monte-Carlo quadrature to converge faster than one using the same amount of pseudorandom samples.
A commonly used sequence is the Sobol sequence, which is also used in this thesis.
It is generated using the Antonov and Saleev variant, which uses the Gray code of $i$ to construct the $i$-th sample.

\subsection{Inverse transform sampling}
To extend the generation pseudorandom samples to non-uniform distributions, one commonly used approach are inverse transformations.
\begin{definition}[Inverse transformation]
Given a cumulative distribution function $F_X$ that describes the distribution of the random variable $X$, we define the transformation $T_X$ for the random variable $X$ with
\begin{equation}
T_X \colon [0,1] \mapsto \mathds{R}, ~ T_X(U)=X
\end{equation}
where $U \sim \mathcal{U}[0,1]$ is a uniformly distributed random variable.
\end{definition}
This transformation is then exactly the inverse of $F_X$ with $T_X=F_X^{-1}$, hence the name inverse transform sampling.
Unfortunately, the inverse $F_X^{-1}$, also called the quantile function, does not always have a closed-form solution.
There exist many different algorithms to approximate the quantile function for commonly used distributions.
The variant that is used in this thesis is presented in \cite{} and is basically a combination of different algorithms that excel for specific ranges of values for $U$.
The rest of the implementation just needs to generate a set of a quasirandom samples from $\mathcal{U}[0,1]^d$ and apply the quantile function on the samples componentwise to get the quasirandom samples distributed according to the multivariate distribution $\rho(x)=\prod_{i=1}^d \rho_i(x_i)$.

\section{Error mesaures}

To evaluate the quality of a surrogate $\hat{f}$, we define a couple of error metrics.
An established and commonly used error metric for function approximation is the $L_2$ error:
\begin{definition}[$L_2$ error]
Let
\begin{equation}
||g||_{L^p}^p \coloneqq \int_{\Omega} |g(x)|^p \text{d}x
\end{equation}
be the $L^p$ norm for real functions $g \colon \Omega \mapsto \mathds{R}$.
We then define
\begin{equation}
\varepsilon_{\text{$L^2$}}(\hat{f})=\|f - \hat{f}\|_{L_2}=\sqrt{\int_{\Omega} |f(x) - \hat{f}(x)|^2 \text{d}x}
\end{equation}
as the $L^2$ surrogate approximation error.
\end{definition}

In statistics, to evaluate the accuracy of an estimator, the mean squared error and the root mean squared error are commonly used.
They can be described as the expected value of the squared errors and the square root of the expected value of the squared errors:
\begin{definition}[Mean squared error]
Let $\hat{f}$ be the surrogate for the model function $f$ and $S \subseteq \Omega$ a sample with $|S|=n$.
We then define
\begin{equation}
\varepsilon_{\text{MSE}}(\hat{f}) = \frac{1}{n} \sum_{i=1}^n (f(x_i) - \hat{f}(x_i))^2
\end{equation}
as the mean squared error for the surrogate based on the given sample.
\end{definition}

\begin{definition}[Root mean squared error]
Let $\hat{f}$ be the surrogate for the model function $f$ and $S \subseteq \Omega$ a sample with $|S|=n$.
We then define
\begin{equation}
\varepsilon_{\text{RMSE}}(\hat{f}) = \sqrt{\varepsilon_{\text{MSE}}(\hat{f})} = \sqrt{\frac{1}{n} \sum_{i=1}^n (f(x_i) - \hat{f}(x_i))^2}
\end{equation}
as the root mean squared error for the surrogate based on the given sample.
\end{definition}

The RMSE is closely related to the definition of the continous $L_2$ error \ref{}, as it is just a discrete version of it.
It holds that $\lim_{n \to \infty} \varepsilon_{\text{RMSE}}(\hat{f}) = \varepsilon_{\text{$L_2$}}(\hat{f})$.


Absolute error metrics have the problem of being sensitive to the size of the function values.
To allow for a comparision of the results between different model functions that have a different range of function values, we also define normalized errors:
\begin{definition}[Normalized root mean squared error]
Let $\hat{f}$ be the surrogate for the model function $f$ and $S \subseteq \Omega$ a sample with $|S|=n$.
We then define
\begin{equation}
\varepsilon_{\text{NRMSE}}(\hat{f}) = \frac{\varepsilon_{\text{RMSE}}(\hat{f})}{\max_{x \in S} f(x) - \min_{x \in S} f(x)}
\end{equation}
as the normalized root mean squared error for the surrogate based on the given sample.
\end{definition}


\chapter{Input transformations}
\label{chap:c3}

A lot of work has already gone into exploring all kinds of variants and optimizations for sparse grids, which resulted in many different families of basis functions, grid point structures, and algorithms, such that the general sparse grids technique can be applied in a better and more flexible to many different kinds of problems.
However, the primary focus lied on optimizing sparse grids itself and less on inspecting and adapting to the inputs $x \in \Omega$ itself.
The basic idea that is propagated in this thesis is that instead of directly using the sparse grids to interpolate a function with $f(x) \approx \hat{f}(x), ~ \hat{f} \in V^{\text{s}}_{\ell}$, we insert another layer of flexibility into the process by introducing an input transformation function $t$ and creating the surrogate $\hat{f}$ in tandem with $t$ such that the transformed approximation $f(x) \approx \hat{f}(t(x))$ has the potential to perform better.
This general idea and also its details can and will be expanded upon in this chapter.

\section{Intrinsic dimension}

There already exist several concepts used in signal processing and data science that are related to the idea of input transformations to exploit certain characteristics of the input data like PCA \cite{} or the concept of intrinsic dimension \cite{}.
The intrinsic dimension of a function or dataset is the minimal amount of features required to completely represent its output.
It can be used as a guidance to determine a lower bound on the complexity of the model function and also optionally exploit the knowledge to construct a lower-dimensional surrogate and thus perform a dimension reduction.

\begin{definition}[Intrinsic dimension]
A $d$-dimensional function $f(x_1, \dots, x_d)$ has intrinsic dimension $r \leq d$ if there exists a transformation function $t \colon \Omega \mapsto \Omega_r$, such that
\begin{equation}
\forall x \in \Omega \colon f(x)=f_t(t(x))
\end{equation}
\label{def:intrinsic}
\end{definition}

Even though \cref{eq:intrinsic} does not specify $\Omega_r$, i.e. it can be an arbitrary subspace $\Omega_r \in \mathds{R}^r$, we assume that the transformation is bounded and that $\Omega_r$ therefore is also bounded and can be  scaled w.l.o.g. to a unit hypercube $\Omega_r=[0,1]^r$.

We define the minimal intrinsic dimension of a function as the optimal intrinsic dimension $r^\ast$.
However, even if we are able to find a non-optimal $r$-dimensional transformation with $r > r^\ast$ that satisfies the requirement of \cref{eq:intrinsic}, we still have gained an advantage and can define that function as having an intrinsic dimension of at most $r$.
Even if it holds that $r=r^\ast$, we still call it as having an intrinsic dimension of at most $r$, since we usually don't know the actual intrinsic dimension $r^\ast$ without a proof.
Since we are dealing with function approximation problems in this thesis, we extend definition \cref{eq:intrinsic} to also allow for approximation errors.

\begin{definition}[Approximate intrinsic dimension]
Let $f$ be $d$-dimensional function, $\varepsilon$ an error metric.
We then define $f$ as having an approximate intrinsic dimension of $r \leq d$ with an error of $e$ if there exists a $t \colon \Omega \mapsto \Omega_r$, such that
\begin{equation}
e=\varepsilon\left(f(x) - f_t(t(x))\right)
\end{equation}
\end{definition}

Similar to the optimal intrinsic dimension $r^\ast$, we are not concerned about the finding also do not know the optimal error $e^\ast= \argmin_{t, f_t} \epsilon(f(x) - f_t(t(x)))$ for a given $r$.
With the introduction of approximations into the intrinsic dimension definition, there is also room for a tradeoff between  intrinsic dimension and approximation error.
The main goal to effectively make use of lower dimensional function approximations is therefore finding functions $t$ and $f_t$ that provide a good tradeoff between dimension and approximation error.

\section{Motivation}

All grid-based interpolation methods all share the common problem of being affected by the curse of dimensionality to some degree.
This means that as the dimension of the models grow, the amount of grid points and the model function evaluations required to generate a sparse grid grow too fast and therefore put a limit on how many dimensions are feasible.
Sparse grids itself are already an effective strategy to reduce the consequences the curse of dimensionality compared to full grids, as they provide a good tradeoff between the amount of grid points used and the interpolation accuracy compared to full grids, but they only weaken and delay the effects of the curse of dimensionality.
Therefore, in higher dimensions, employing some strategy for dimension reduction is necessary.
Of course, dimension reduction strategies are limited by the minimum amount of accuracy that is required for the specific use case, since reducing the amount of grid points usually reduces accuracy and is therefore a tradeoff.
In this chaper we show a few already established approaches to sparse grid based dimension reduction and look at their advantages and disadvantages.

The first example model function $f_1$ to investigate the properties of the presented methods, is very simple designed to be a good fit for grid based methods.
It is a two-dimensional function, where only the first component and a constant term contribute to the total function value while the second component does not:

\begin{equation}
f_1(x_1, x_2)=\sin(2 \pi x_1) + 1
\nonumber
\end{equation}

One can easily see that it is an intrinsically one-dimensional function according to \cref{eq:intrinsic}, since we a transformation function could just cut off the second component.
However, just by rotating the function by a few degrees, we obtain
\begin{equation}
f_2(x_1,x_2)=\sin(2 \pi x_1') + 1, ~~~~~ x_1'=\cos(0.15 \pi) x_1 -\sin(0.15 \pi) x_2,
\end{equation}
also a two-dimensional function that is still intrinsically one-dimensional.

\subsection{Spatially adaptive sparse grids}

Both functions can be approximated using spatially adaptive sparse grids.
We can then investigate how their generated grid points and interpolation errors compare.

\begin{mdframed}[style=style]
\vspace{2.5mm}
\begin{figure}[H]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{graphics/grid_f1}
  \captionof{figure}{Grid points for $f_1$}
  \label{fig:grid_f1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{graphics/grid_f2}
  \captionof{figure}{Grid points for $f_2$}
  \label{fig:grid_f2}
\end{subfigure}
\vspace{2.5mm}
\delimit
\captionof{figure}{Grid points of generated spatially adaptive sparse grids for $f_1$ and $f_2$ that started out as a regular sparse grid with level $l=1$. It used the surplus refinement criterion, 15 refinements steps with one grid point refinement per step, and modified linear basis functions.}
\label{fig:grids}
\end{figure}
\end{mdframed}


\Cref{fig:grids} shows how spatially adaptive sparse grids are affected by the different alignment of function terms.
The spatially adaptive strategy is able to exploit the one-dimensional nature of $f_1$ by only refining along the first dimension and therefore spending the grid points where they matter.
However, the intrinsic one dimensional term can not be exploited for $f_2$, as it is no longer axis-aligned.
Note that due to the default refinement rule that add all child grid points, the upper and lower grid point rows in \cref{fig:grid_f1} are created but are not necessary.
A tweaked refinement rule could fix this.

\subsection{Radial basis functions}
Radial basis functions behave differently compared to sparse grids when being applied to the same two functions, as their basis functions are not constructed using the tensor-product approach.
Instead they are orientation agnostic, as only the radius relative a support point determines the basis function value.
Therefore, as seen in figure \cref{fig:grid_rbf_errors}, the approximation errors stay pretty much identical in both cases.
However, the general approximation error is worse than the sparse grid errors, as radial basis functions are not suited for approximating functions with a constant dimension when being compared to modified basis functions, which come with a constant component well suited for that.

\begin{mdframed}[style=style]
\begin{figure}[H]
  \centering
  \includegraphics[width=.8\linewidth]{graphics/rbf_errors}
  \caption{Spatially adaptive sparse grid points for $f_1$}
  \delimit
\captionof{figure}{Results of ANOVA.}
  \label{fig:grid_rbf_errors}
\end{figure}
\end{mdframed}

\subsection{ANOVA}


The same behaviour of drastically different results for both functions when using spatially adaptive sparse grids can also be observed for other sparse grid based methods, like ANOVA.


\begin{mdframed}[style=style]
\vspace{2.5mm}
\begin{figure}[H]
\begin{subfigure}{.5\textwidth}
  \centering
  \begin{tabular}{ l c c }
\hline \hline
& $2 \notin C~$ & $2 \in C$ \\
\hline
$1 \notin C$ & ? & 0.0\\
$1 \in C$ & ? & 0.0\\
\end{tabular}
  \caption{Distribution of the variance of $f_1$ between the ANOVA components.}
  \label{fig:anova_f1}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \begin{tabular}{ l c c }
\hline \hline
& $2 \notin C~$ & $2 \in C$ \\
\hline
$1 \notin C$ & ? & ?\\
$1 \in C$ & ? & ?\\
\end{tabular}
  \caption{Distribution of the variance of $f_2$ between the ANOVA components.}
  \label{fig:anova_f2}
\end{subfigure}
\delimit
\captionof{figure}{Results of ANOVA.}
\label{fig:anova}
\end{figure}
\end{mdframed}


The ANOVA method decomposes the functions into four terms.
A constant term $f_\emptyset$, two one-dimensional terms $f_{\{1\}}$ and $f_{\{2\}}$, and a two dimensional term $f_{\{1,2\}}$.
As we can see in \cref{fig:anova}, at least for the function $f_1$, the method correctly identifies that only the terms $f_\emptyset$ and $f_{\{1\}}$ have some output variance with $\sigma^2(f_\emptyset)=? > 0$ and $\sigma^2(f_{\{1\}}=? > 0$.
Therefore the other terms can be dropped and the function can be completely represented using only a one-dimensional function.
However, we can observe the same problem as with spatially adaptive sparse grids, i.e. the ANOVA method fails to identify that the function $f_2$ is still intrinsically one-dimensional as seen in figure \ref{fig:anova_f2} since for all ANOVA components it holds that $\sigma^2(f_{c}> 0$ and the ANOVA method would report that no dimension reduction can be applied.

\subsection{Summary}

A fundamental property of any axis-aligned mesh-based method that uses basis functions created by using the tensor product approach is that their quality depends on whether the model function primarily also consists out of axis-aligned terms.
In contrast, other basis functions like radial basis functions do not depend on the alignment of the function terms they are trying to approximate, i.e. they can be seen as orientation agnostic.
In turn this means that an effective dimension reduction with conventional sparse grids based methods is highly dependent on the alignment of the model function.
Without adding another layer of flexibility with an input transformation function, sparse grids can not use their full potential in many cases, as we have seen with function $f_2$.

\section{Transformed sampling}

To achieve a good convergence rate and general representativeness of a drawn set of samples, e.g. for quadrature, it is vital to achieve low-discrepancy and cover the input space $\Omega$ more evenly.
Under normal circumstances, to achieve the same discrepancy in a high-dimensional space as in a low-dimensional space,
exponentially more samples are required.
However, combining this requirement with the concept of intrinsic dimension \cref{def:intrinsic}, we observe that we effectively only need to cover $\Omega_r$ and adapt any operation that normally works on $\hat{f}$ to instead work on the lower dimensional function $\hat{f_t}$.
The main advantage therefore is that the lower amount of samples required to achieve a good coverage of the definition space of $f_t$ mainly depends on $r$ and not on $d$.

\begin{definition}[Transformed distribution]
Let $t^{-1}(x_{t})=\{x \in \Omega \mid t(x)=x_{t}\}$ be the inverse transformation function.
We then define the transformed distribution as
\begin{equation}
\rho_t \colon \Omega_r \to \mathds{R_+}, ~~ \rho_t(x_r)=\int_{t^{-1}(x_r)} \rho(x) \; \text{d}x 
\end{equation}
This is also a distribution since it holds that
\begin{equation}
\int_{\Omega_t} \rho_r(x_t) \; \text{d}x=\int_{x_r \in \Omega_r} \int_{t^{-1}(x_r)} \rho(x) \; \text{d}x = \int_{\Omega} \rho(x) \; \text{d}x = 1
\end{equation}
\end{definition}

This means that the effective distribution type will change and for example a uniform distribution $\rho$ will often be transformed into a non-uniform distribution $\rho_t$.
Furthermore, the quality and properties of transformed low discrepancy sequences changes compared to its original sequence \cite{}.
To effectively sample from $\rho_t$, it is still easier to just draw the samples $x_1, \dots, x_n$ from $\rho$ directly and feed them into the transformation function to obtain $(t_1, \dots, t_n)=(t(x_1), \dots, t(x_n))$.

\section{Linear hypercube transformations}

As the concept of intrinsic dimension makes it possible to, at least in theory, use arbitrary transformation functions, there are many possible general types of transformation functions to choose from.
The previous chapter showed how a simple change of basis of the function inputs, i.e. an orthogonal transformation, can have a huge impact on the sparse grid surrogate construction.

Motivated by this insight, the main approach that will be used and investigated in this thesis starts out with finding an orthonormal matrix $W \in \mathds{R}^{d \times d}$ that is aligned along the active directions of the model function in the best possible way.
This matrix $W$ is referred to as the active direction matrix, as the column vectors $w_i$ point into directions in which the function output has a high rate of change and are ordered by the level of change.
It should hold that $w_1$ is the most active direction, while $w_d$ is the most inactive direction.
Since all column vectors $w_i$ have to be orthogonal, they can not point into arbitrary directions and are limited by other active directions.
Therefore, a good active direction matrix ...

We then go one step further and also try to eliminate the most inactive directions of $W$, effectively performing a dimension reduction.
After performing a change of basis with $W$ on the inputs to obtain $W^T x$, we effectively eliminate the most inactive dimensions, as inactive directions are now mapped to inactive dimensions.
This results in the possibility of drastically reducing the amount of grid points while only suffering a small increase in error in theory, as only the most unimportant dimensions, which should not contribute a lot to the approximation error, are eliminated.

\begin{definition}[Transformation matrix]
Let $W \in \mathds{R}^{d \times d}$ be the active direction matrix with $W^T W=I$, and let $r \leq d$ be the reduced dimension.
We then define
\begin{equation}
W_r = \begin{bmatrix}
  \\
    w_1 ~ w_2 ~ \dots ~ w_{r-1} ~ w_r\\
    \\
  \end{bmatrix}
\end{equation}
as the transformation matrix.
\end{definition}

As we can see, the transformation matrix is simply obtained from a given active direction matrix $W$ and a reduced dimension $r$ by cutting off the last $d - r$ directions from $W$.
For now, we assume that the $W$ and $r$ are given, but we will cover the process of determining those essential inputs later on in detail.

\subsection{Unit hypercube projections}

To construct a transformation function $t$ that maps from the $d$-dimensional unit hypercube $\Omega=[0,1]^d$ to the $r$-dimensional unit hypercube $\Omega_r=[0,1]^r$ for a given transformation matrix $W_r$, several adjustments have to be made in order to obtain a usable transformation function as the naive approach with $W_r^T x$ would map inputs outside of $\Omega_r$ in certain cases.
When applying a linear transformation $W_r^T x$ on the elements of a hypercube $x \in \Omega$, the result can be described with the help of zonotopes:

\begin{definition}[Zonotope]
Let $l$ be the dimension of the zonotope, and let $k \geq l$ be the amount of zonotope generators.
A $l$-dimensional zonotope $Z$ with $k$ generator vectors $g_1, \dots g_k \in \mathds{R}^l$ corresponds to the $l$-dimensional minkowski sum
\begin{equation}
Z=\left\{\sum_{i=1}^k g_i x_i \mid ~ -1 \leq x_i \leq 1\right\}.
  \label{zonotope}
\end{equation}
\end{definition}


\begin{mdframed}[style=style]
\begin{figure}[H]
\centering
  \includegraphics[width=0.5\linewidth]{graphics/zonotope}
  \delimit
  \captionof{figure}{Exemplary creation of a Zonotope $Z$ with $l=2$, $k=3$ and the three generators $g_1,g_2,g_3$.}
  \label{fig:zonotope}
\end{figure}
\end{mdframed}


When applying the projection $W_r^T$ to the inputs $x \in \Omega$, we can observe that
\begin{equation}
\begin{split}
W_r^T x=~W_r^T \sum_{i=1}^d x_i e_i=~\sum_{i=1}^d x_i p_i , ~~~~ p_i=W_r^T e_i,
\nonumber
\end{split}
\end{equation}
where the $p_i$ are just the projected unit vectors.
Therefore, since for all components are in the unit interval with $x_i \in [0,1]$, we can express the transformed inputs with
\begin{equation}
\{W_r^T x \mid x \in \Omega\}=\left\{\sum_{i=1}^d x_i p_i \mid x \in \Omega \right\}=\left\{ \sum_{i=1}^r x_i p_i \mid 0 \leq x_i \leq 1\right\}.
\label{wtx}
\end{equation}
To get \cref{wtx} closer to the zonotope definition in \cref{zonotope}, we first center the inputs $x \in \Omega$ before applying the projection and divide the generators $p_i$ by 2.
For this, let $c_i=(\frac{1}{2}, \dots, \frac{1}{2})^T\in \mathbb{R}^i$ be the center of the $i$-dimensional unit hypercube.
We can express this with
\begin{equation}
\begin{alignedat}{2}
&\left\{W_r^T (x - c_d) \mid x \in \Omega\right\}&&=\left\{W_r^T \sum_{i=1}^d \left(x_i - \frac{1}{2}\right) e_i \mid x \in \Omega \right\}\\
=&\left\{\sum_{i=1}^d \left(x_i - \frac{1}{2}\right) W_r^T e_i \mid x \in \Omega \right\}&&=\left\{\sum_{i=1}^d \left(x_i - \frac{1}{2}\right) w_i \mid x \in \Omega \right\}, ~~ w_i=W_r^T e_i\\
=&\left\{\sum_{i=1}^d \left(x_i - \frac{1}{2}\right) w_i \mid 0 \leq x_i \leq 1 \right\}&&=\left\{\sum_{i=1}^d x_i w_i \mid -\frac{1}{2} \leq x_i \leq \frac{1}{2} \right\}\\
=&\left\{\sum_{i=1}^d \frac{1}{2} x_i w_i \mid -1 \leq x_i \leq 1 \right\}&&=\left\{\sum_{i=1}^d x_i p_i \mid -1 \leq x_i \leq 1 \right\}, ~~ p_i=\frac{1}{2} w_i
\label{zonotope_form}
\end{alignedat}
\end{equation}

Therefore, defining the projection function with
\begin{equation}
p \colon \Omega \mapsto Z, ~~ p(x)=W_r^T (x-c_d)
\nonumber
\end{equation}
results in a projected input space that has the zonotope form of \cref{zonotope} with the $p_i$ are generators.
The projected zonotope is then
\begin{equation}
Z_{p}=\left\{\sum_{i=1}^r p_i x_i , ~ -1 \leq x_i \leq 1\right\}, ~~ p_i=\frac{1}{2} W_r^T e_i,
\nonumber
\end{equation}
where according to \cref{zonotope_form}, the projected input space is exactly the zonotope with $Z_{p}=p(\Omega)$.

\subsection{Surrogate space}

However, since the surrogate creation process expects inputs from a hypercube and not a zonotope, the zonotope $Z_p$ is first encased in an $r$-dimensional hypercube called the surrogate space $S$.
The surrogate space is then later scaled and aligned to obtain the unit hypercube $\Omega_r$.
Generators $q_1, \dots, q_r$ of the encasing hypercube can be calculated by applying the Gram–Schmidt process on the ordered zonotope genrators $p_i$ to first obtain the orthogonal vectors
\begin{equation}
q_i'=p_i - \sum_{k=1}^{i-1} \frac{\langle q_k, p_k\rangle}{\langle q_k, q_k\rangle}
\label{q}
\end{equation}
and then the appropriately scaled vectors
\begin{equation}
q_i=s_i \frac{q_i'}{\| q_i' \|},~~ s_i=\sum_{k=1}^d \langle q_i, p_k\rangle.
\label{q_scaled}
\end{equation}
The surrogate space $S$, which encases the zonotope $Z$, is then an $r$-dimensional hypercube with
\begin{equation}
S=\left\{\sum_{i=1}^r q_i x_i, ~ -1 \leq x_i \leq 1\right\}
\label{surrogate_space}
\end{equation}
where the scaling factors $s_i$ are chosen in such a way that the hypercube $S$ exactly matches the extent of the zonotope $Z_p$, as shown in \cref{surrogate_space}.

\begin{mdframed}[style=style]
        \centering
\begin{minipage}{.49\textwidth}
        \centering
  \captionof{figure}{Construction of the encasing surrogate space $S$ for the zonotope $Z_p$ of \cref{fig:zonotope} according to \cref{surrogate_space}.\\
  The projected zonotope $Z_p$ is drawn in \lightblue, the encasing surrogate space $S$ in dotted \darkblue, and the origin in \red.}
    \end{minipage}%
    \begin{minipage}{0.49\textwidth}
        \centering
        \vspace{3.5mm}
  \includegraphics[width=0.8\linewidth]{graphics/surrogate_space}
  \hspace{-5.5mm}
  \label{fig:surrogate_space}
    \end{minipage}
\end{mdframed}

As we only need $r$ generators for the surrogate space, but have $d$ generators of $Z_p$ available, not all $p_i$ are used in the surrogate space construction.
In theory, the generators $p_i$ that are used to calculate the $q_i$ are not ordered by default and also don't have to be ordered.
However, to emphasize the most important generators $p_i$, the generators $p_i$ are ordered by their magnitude w.l.og. $|p_1|\geq \dots \geq |p_d|$.
This should result in the best possible encasement of $Z_p$.

\subsection{Reduced unit hypercube}

The surrogate space $S$, which is already a hypercube, can then be easiliy transformed into the unit hypercube $\Omega_R$, as shown in \cref{fig:aligned}.
First, we observe that the generators $q_i$ of $S$ with
\begin{equation}
Q=\begin{bmatrix}
  \\
    q_1 ~ q_2 ~ \dots ~q_{r-1} ~ q_r\\
    \\
  \end{bmatrix}
  , ~~
\label{alignment}
\end{equation}
are orthogonal.
We can perform a change of basis on every element that has been projected into $Z_p$, and therefore also $S$, to obtain $v=Q^T p(x)$.
Per surrogate space definition \cref{surrogate_space}, we know that $v_i \in [-1,1]$ holds as $v \in S$.
Once the projected inputs are represented by the new basis $Q$, it is therefore trivial to transform the surrogate space into an $r$-dimensional unit hypercube $\Omega_r$ by first scaling it down by $2$ and then uncenter it again to finally obtain the transformation function
\begin{equation}
t_{W_r}(x)= c_r + \frac{1}{2} Q^T p(x).
\label{linear_trans}
\end{equation}

\begin{mdframed}[style=style]
\begin{figure}[H]

        \centering
\begin{minipage}{.49\textwidth}
        \centering
  \captionof{figure}{Transformation of the surrogate space $S$ from \cref{fig:surrogate_space} into the unit hypercube $\Omega_r$ according to \cref{alignment}.\\
  The projected zonotope $Z_p$ is drawn in \lightblue, the encasing surrogate space $S$, which is now equal to $\Omega_r$ in dotted \darkblue, and the origin in \red.}
    \end{minipage}%
    \begin{minipage}{0.49\textwidth}
        \centering
   \includegraphics[width=0.8\linewidth]{graphics/surrogate_space_unit}
  \label{fig:aligned}
    \end{minipage}

\end{figure}
\end{mdframed}


\subsection{Visualizing transformations}
\label{sec:vis_trans}

The described transformation process can be visualized by illustrating where grid points from the lower dimensional space $\Omega_r$ would reside in the original input space $\Omega$. 
While it is normally not required to revert the previously shown transformation process, i.e. going from $\Omega_r$ back to $\Omega$, we require it for the visualization of transformed surrogates.

We observe that \cref{linear_trans} can be rearranged to solve for $p(x)$ to get
\begin{equation}
p(x)=2 (Q^T)^{-1} (t_{W_r}(x) - c_r)
\label{proj}
\end{equation}
Since the column vectors of $Q$ are defined as orthogonal in \cref{q}, $Q$ can be expressed as $Q=Q_n \text{diag}\left(s_1, \dots, s_r\right)$ where the column vectors of $Q_n$ are just the normalized column vectors of $Q$ with $(Q_n)_i=\frac{q_i}{\| q_i \|}$ and the $s_i=\| q_i \|$ are the scaling factors from \cref{q_scaled}.
Therefore, it holds that $Q^T=\text{diag}\left(s_1, \dots, s_r\right) Q_n^T$ and
\begin{equation}
\begin{split}
\left(Q^T\right)^{-1}=~&\left(Q_n^T\right)^{-1}  \text{diag}\left(s_1, \dots, s_r\right)^{-1}  \\
=~&Q_n \text{diag}\left(\frac{1}{s_1}, \dots, \frac{1}{s_r}\right) .
\label{qtinv}
\end{split}
\end{equation}
We can then insert \cref{qtinv} into \cref{proj} to obtain
\begin{equation}
\begin{split}
p(x)=&~2 ~ Q_n \text{diag}\left(\frac{1}{s_1}, \dots, \frac{1}{s_r}\right) (t_{W_r}(x) - c_r)\\
=&~Q_n \text{diag}\left(\frac{2}{s_1}, \dots, \frac{2}{s_r}\right)  (t_{W_r}(x) - c_r).
\end{split}
\end{equation}

To reverse the transformation completely, we have to deal with reversing $p(x)$ and the fact that in many cases, the inverse $p^{-1}(x_p)=\{x \in \Omega \mid p(x)=x_{p}\}$ function is not injective, because $|p^{-1}(x_p)| > 1$.
Our goal is to reverse the transformation only onto certain representative elements of the equivalence classes $[x]_p=\{x \in \Omega \mid p(x)=x_{p}\}$.
We then only consider the points
\begin{equation}
C=\{x - c_d  \in \text{span} ~ W_r \mid x \in \Omega\},
\label{proj_rep}
\end{equation}
i.e. who lie in the vector subspace that is spanned by the active directions in $W_r$ and crosses the center $c_d$.
Next, we reverse the projection $p(x)$ only onto the representative elements $x \in C$, so that we are able to map one element $x_r\in \Omega_r$ onto one element $x \in C$ to obtain $t_{W_r}^{\text{reverse}}\colon \Omega_r \mapsto C$ with
\begin{equation}
t_{W_r}^{\text{reverse}}(x_r)=~W_r Q_n \text{diag}\left(\frac{2}{s_1}, \dots, \frac{2}{s_r}\right) (x_r - c_r) + c_d
\end{equation}

In \cref{fig:trans_vis} we assume a transformation with $r=2$ and $d=3$, and reverse the grid points of a regular sparse grid of level $4$? lying in $\Omega_r$ onto a plane in the original three dimensional space $\Omega$ with the help of equation \cref{proj} and \cref{proj_rep}.

\begin{mdframed}[style=style]
\begin{figure}[H]
\centering
  \includegraphics[width=.7\linewidth]{graphics/surrogate_vis}
  \delimit
  \captionof{figure}{Visualization of a transformation with $r=2$ and $d=3$. The grey background represents the outline of $\Omega$, the arrows on the sparse grid the first two column vectors of $W_r$ that span the plane on which the sparse grid lies.}
  \label{fig:trans_vis}
\end{figure}
\end{mdframed}

\section{Active Subspaces}

The method of active subspaces \cite{CG15} aims to identify the most influential directions in the parameter space to construct a lower-dimensional subspace of $\Omega$ that covers most of a models output variance to conduct parameter studies with a reduced amount of dimensions.
In constrast to conducting parameter studies, we will use the so-called active directions of a computed active subspace to construct the transformation matrix $W_r$.
Let
\begin{equation}
C = \int_{\Omega} (\nabla f) (\nabla f)^T \rho ~ dx
\label{eq:as_c}
\end{equation}

be the average outer product of the gradient, a $(d \times d)$ matrix.
Since $C$ is a positive semi-definite matrix, it is possible to decompose it into its eigenvectors $v_i$ and their corresponding real eigenvalues $\lambda_i$ with

\begin{equation}
C = V \Lambda V^T, ~~ \Lambda = diag(\lambda_1, ..., \lambda_d), ~~ V=
  \begin{bmatrix}
  \\
    v_1 ~ v_2 ~ \dots ~ v_{d-1} ~ v_d\\
    \\
  \end{bmatrix}.
\nonumber
\end{equation}

We furthermore assume that the eigenvectors in this decomposition are ordered, i.e. $\lambda_1 \geq ... \geq \lambda_d$.
A larger eigenvalue indicates a higher rate of change along the direction, more precisely it is the average squared directional derivative of f with respect to its eigenvector $v_i$ \cite{CG14} with

\begin{equation}
\lambda_i=\mathds{E}[((\nabla f)^T v_i)^2]
\label{eigenvalues}
\end{equation}

Therefore, the column vectors of the matrix $V$ are ordered from most the active direction $v_1$ to least the active direction $v_d$.
By keeping only a specific amount of the most active directions $r \leq d$ we obtain the transformation matrix

\begin{equation}
W_r=\begin{bmatrix}
  \\
    v_1 ~ v_2 ~ \dots ~ v_{i-1} ~ v_r\\
    \\
  \end{bmatrix}
\label{basis}
\end{equation}

In the context of this thesis, the given input sample $\{x_1, \dots, x_n\}$ is used to derive the matrix $C$ from \cref{eq:as_c} using a classical Monte-Carlo approach from \cite{}.
The matrix $C$ can then be approximated with
\begin{equation}
C \approx \frac{1}{n} \sum_{i=1}^n  (\nabla f(x_i)) (\nabla f(x_i))^T
\label{eq:as_c_approx}
\end{equation}

\subsection{Estimating gradients for active subspaces}

One superficial downside of the active subspace method is that gradients are required to approximate the matrix $C$ in \cref{eq:as_c_approx}.
In cases where the gradient function is not known and can not be easily computed, this would seem like a limiting factor is the feasability of active subspaces.
However, as this section will show, the gradients can be approximated in various ways from the available input data in ways that allows for a acceptable active subspace approximation.
The approximated gradients are then used as inputs for equation c\ref{eq:as_c_approx} to obtain the approximated active subspace matrix.
There are various different methods of achieving that, where each method has its advantages and disadvantages.

\begin{definition}[$L^2$ norm]
Let $v \in \mathds{R}^{d}$ be a vector. Then
\begin{equation}
\| v\|_2=\sqrt{\sum_{i=1}^d |v_{i}|^2}
\nonumber
\end{equation}
is the $L^2$ norm of the vector $v$.
\end{definition}

To judge the quality of approximated gradients, we first have to define an error metric for them.

\begin{definition}[Average gradient error]
Let $S=\{x_1, \dots, x_n\}$ be an input sample, and let $\widetilde{\nabla} f(x_i)$ the approximated gradient for each sample point $x_i$. We then define
\begin{equation}
\varepsilon_{\text{grad}}(S)=\frac{1}{n} \sum_{i=1}^n \frac{\| \widetilde{\nabla} f(x_i) - \nabla f(x_i) \|_2}{\| \nabla f(x_i) \|_2}
\nonumber
\end{equation}
as the average gradient approximation error of the sample.
\end{definition}

\begin{definition}[Frobenius norm]
Let $A \in \mathds{R}^{m \times n}$ be a matrix. Then
\begin{equation}
\| A\|_2=\sqrt{\sum_{i=1}^m \sum_{j=1}^n |a_{i,j}|^2}
\nonumber
\end{equation}
is the Frobenius norm of the matrix $A$.
\end{definition}

The most basic error metric for an estimated active subspace matrix would be calculating the Frobenius norm of the approximation error compared to the true active subspace, i.e. to calculate $\| W - W^\ast \|_2$.
However, this error calculation is not suitable for cases in which $\lambda_i \approx 0$, since the corresponding eigenvectors can vary greatly in that case.
It has therefore to be adapted by omitting the affected eigenvectors, which are just the most inactive directions, from the calculation if such a case occurs, i.e. calculating $\| W_r - W_r^\ast \|_2$ instead with some $r$ with $\lambda_r \napprox 0$.

\begin{definition}[Active subspace error]
Given an estimated active subspace matrix $W_r$ and the reference matrix $W_r^*$, the
active subspace error is defined by
\begin{equation}
\varepsilon_{\text{AS}}(W_r)=\frac{\| W_r - W_r^* \|_2}{\| W_r^* \|_2}.
\nonumber
\end{equation}
\end{definition}

In the context of active subspaces, the approximation error of the gradients is not the relevant metric to evaluate a method.
Instead, the primary focus is the active subspace approximation error, which is decoupled from the individual gradients, as it is the only metric that matters in the end when working with active subspaces.
Even though $\varepsilon_{\text{AS}}$ is entirely based on the average outer product of the gradients, there is no strong link between $\varepsilon_{\text{grad}}$ and $\varepsilon_{\text{AS}}$.
While the individual gradients approximated by different methods can be far off, the average of the outer products of the gradients from \cref{eq:as_c_approx} can still come close to the real one if the general trend of the gradients is still be reflected in the approximated gradients.
Therefore, inaccuracies of single gradients can be averaged out but systematic biases of the gradients will influence the approximated active subspace.

\subsection{Finite differences}

The most common approach to estimate the gradients are finite differences.
In this thesis, an adaptive mix of forward and backward differences with a fixed distance $h$ are used, where the range of possible spacing is constrained by $0 < h \leq 0.5$:

\begin{equation}
\frac{\partial f(x)}{\partial x_i} \approx
\begin{cases}
    \dfrac{f(x_1, \dots, x_i + h, \dots, x_d) - f(x)}{h}, & x_i + h \leq 1 \\[1.5em]
    \dfrac{f(x_1, \dots, x_i - h, \dots, x_d) - f(x)}{h}, & \text{else}
\end{cases}
\end{equation}

The downside in the context of the sample-based approach is the requirement to additionally evaluate the model function at $d$ other inputs to determine the gradient at one sample point.
For high $d$ and a large amount of samples $n$, $dn$ function evaluations for finite differences may become too costly.

\subsection{Directional derivatives}

In cases where finite differences are not suitable, the gradients can be determined using a different approach that only works on a given input sample and does not require further function evaluations.
The approach is to roughly approximate the gradient by looking at neighboring points in the same sample and using directional derivatives to create a system of linear equations for the gradient at a certain input.
Given two inputs $x, y \in \Omega, x \neq y$, we define the distance between the two inputs as $r=y-x$.
By extrapolating the gradient along the direction in a linear fashion, we can define an approximation rule with

\begin{equation}
r \nabla_x f \approx f(y) - f(x)
\end{equation}

Using $m \in \mathds{N}$ neighbours of a sample point $x \in \Omega$ with $y_i \in \Omega \setminus \{x\}, ~ i \in \{1, \dots, m\}$, we can create a system of linear equations with

\begin{equation}
\begin{bmatrix}
    y_1 - x\\
    \vdots \\
    y_m - x
  \end{bmatrix}  \nabla_x f =\begin{bmatrix}
    f(y_1) - f(x) \\ \vdots \\  f(y_m) - f(x)
    \\
  \end{bmatrix}
  \label{eq:dd_sle}
\end{equation}

One limitation of this calculation is the linear nature of the computed gradients, i.e. estimating gradients of a non-linear function can lead to biased gradients.
Furthermore, the choice of the $y_i$ heavily influences the result as well as the size of $m$.
The next sections introduce different ways of choosing the neighbours $y_i$ and also evaluate the results.

\vspace{2mm}
\textbf{Random neighbour approximation~} The random neighbour approximation method takes as an input a subsample $S_{\text{rn}} \subseteq S \setminus \{x\}$ with $|S_{\text{rn}}|=m < n$.
The set $S_{\text{rn}}=\{y_1, \dots, y_{m}\}$ is called a random neighbour set of $x$.
This random neighbour set is then used as an input for equation \cref{eq:dd_sle} to approximate the gradient $\nabla_x f$.

For $n,m \to \infty$, the approximated gradient does not converge to the actual gradients, since the expected distance to the neighbours in a random neighbour set does not go to zero.
The problem of determining average neighbour distance is related to finding the mean line segment length $\Delta (d)$ in a d-dimensional unit hypercube \cite{}.
There is no closed form solution for $\Delta (d)$, but several approximations have been made for various dimensions \cite{}, e.g. $\Delta (1)=\frac{1}{3}$, $\Delta (2)=0.52$, $\Delta (3)=0.66$.

\vspace{2mm}
\textbf{Nearest neighbour approximation~}
The nearest neighbour variant picks a subset $S' \subseteq S \setminus \{x\}$ with a manageable size $n'=|S'|$ similar the the random neighbour method and then calculates the $m \leq n'$ nearest neighbours $S_{\text{nn}}=\{y_1,\dots,y_m\} \subseteq S'$ of the point $x$ with $\|(y_1-x)\|_2 < \|(y_2-x)\|_2< \dots<\|(y_{m-1}-x)\|_2 < \|(y_{m}-x)\|_2$.

Compared to the random neighbour approximation, for $n,n',m \to \infty$, the approximated gradient does converge to the actual gradient, since the expected distance to the neighbours does go to zero.
However, this property comes at the cost of having to sort the available $n'$ neighbours by distance to pick the $m$ closest ones.
The parameters $n'$ and $m$ also have to be chosen carefully, as a unsuitable comination of $n'$ and $m$ would increase the average distance to the neighbours again so that the effect of chosing the nearest neighbours would diminish.


\begin{mdframed}[style=style]
\begin{figure}[H]
  \centering
\begin{subfigure}{.45\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{graphics/as_rn}
  \captionof{figure}{Random neighbour method picks with $m=4$}
  \label{fig:as_rn}
\end{subfigure}%
\begin{subfigure}{.45\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{graphics/as_nn}
  \captionof{figure}{Nearest neighbour method picks with $n'=7$ and $m=4$}
  \label{fig:as_nn}
\end{subfigure}
\delimit
\captionof{figure}{Differences in sample selection between the two neighbour approximation methods for examples with $|S|=10$.
The \darkblue dots are the available samples $S \setminus \{x\}$. A filled dot signals that it is considered when looking for neighbours.
The completely \red circle represents the sample point $x$ and other sample points circled in \red indicate that they were chosen as a neighbour of $x$.}
\label{fig:as_approx}
\end{figure}
\end{mdframed}

\subsection{Approximation quality}

To get an idea of the actual quality of the different methods, especially regarding the resulting active subspace error $\varepsilon_{\text{AS}}$, this section will focus purely on the gradient approximation methods as a to gauge whether the methods can be used to generate input transformations.
Two example functions are investigated for this purpose, wrt. the gradient approximation quality $\varepsilon_{\text{grad}}$ and the subsequent active subspace approximation quality $\varepsilon_{\text{AS}}$.

\vspace{2mm}
\textbf{Simple function~}
The first example function is a simple $3$-dimensional polynomial function:
\begin{equation}
f_1(x)=(0.8 {x_1}^2 - x_2) / (x_3 + 0.5)
\nonumber
\end{equation}



\begin{figure}[H]
\begin{center}
	\includegraphics[width=\textwidth]{graphics/as_errors_f1}
\end{center}
	\captionof{figure}{Mean Active Subspace error calculated with the previously introduced methods compared to the actual Active Subspace for $f_1$ over 10 iterations .}
	\label{fig:as_errors_f1}
\end{figure}

As seen on the left in figure \ref{fig:as_errors_f1}, the finite differences approximation error converges towards zero as the spacing $h$ goes down as expected.
Furthermore, the nearest neighbour method does deliver better results than the random neighbour method wrt. $\varepsilon_{\text{grad}}$.
The random neighbour methods do not converge but are instead in a range that is even worse compared to the finite differences approximation with a very big step size.
Since the mean line segment length is $\Delta(3) \approx 0.66$, the random neighbour method chooses on average neighbours with a distance $|r| \approx 0.66$.
It can be seen that the average gradient error for all random neighbour methods is a little bit worse than finite differences with $h=0.5$, which is consistent with the average distance differences between the two methods.

As a result of the approximation qualities shown on the right in figure \ref{fig:as_errors_f1}, the gradient approximation qualities behave similar.
The random neighbour methods are still in a small range with an error comparable to the finite differences approximation with a very big step size of $h=0.5$.
The nearest neighbour method does deliver far better results than the random neighbour method, as it shows a similar quality as the finite differences approximation with smaller step sizes.
Also, using fewer closer neighbours $m$ for the nearest neighbour method seems to be better.
It can also be observed that the gradient approximation quality does not completely correlate with the active subspace approximation quality.
While for example the nearest neighbour methods have a similar gradient approximation quality as finite differences with $h=0.1$, the active subspace computed by the nearest neighbour method has a far better quality than the finite differences method with $h=0.1$.

\vspace{2mm}
\textbf{More complex function~}
The second case study is a 20-dimensional function:

\begin{align}
\begin{split}
f_2(x_0, \dots, x_{19})=&e^{0.2 x_0 x_1 x_2 x_3 x_4}\\
+ &5 * sin(\pi x_5 x_6 x_7) x_8^2 x_9^5 x_{10}\\
+ &(x_{11} + 2)^2 x_{12}\\
+ &2 x_{13} * 3 x_{14}\\
+ &x_{15} x_{16}\\
+ &log(1 + (10 x_{17} / (0.1 + x_{18} + x_{19})))
\end{split}
\end{align}

It consists out of several terms of varying dimensionality and type, where no term obviously dominates the others.

\begin{figure}[H]
\begin{center}
	\includegraphics[width=\textwidth]{graphics/as_grad_errors_f2}
\end{center}
	\captionof{figure}{Average $L^2$ error of gradients calculated with the previously introduced methods compared to the actual gradients for $f_2$.}
	\label{fig:as_grad_errors_f2}
\end{figure}

As seen in figure \ref{fig:as_grad_errors_f2}, the finite differences approximation error converges towards zero as the step size goes down as expected.

\subsection{Limitations}

Active subspaces are
A larger eigenvalue indicates a higher rate of change along the direction, more precisely it represents the average squared directional derivative of $f$ with respect to its eigenvector $v_i$ with $\lambda_i=\mathds{E}[((\nabla f)^T v_i)^2]$ \cite{CG14}.
However, a high rate of average squared change does not necessarily indicate that a direction $v_i$ is worth keeping.
For example, we can create a simple function with
\begin{equation}
f(x_1, x_2)=10x_1 + \frac{1}{10} \sin(100 \pi x_2)
\end{equation}
that illustrates the limitations of active subspaces.
The optimal active direction worth keeping would be the first component only, as the second term can be seen as a highly changing noise term that does not effect the approximation error if removed, as the amplitude of the sin is small.
However, as the average squared directional derivative along the second dimension $(0,1)^T$ is way higher than along the first one, the calculated active subspace is biased heavily along the second dimension.

A positive aspect of the active subspace approximation methods is that they introduce a regularization through discretization \cite{}, i.e. as the distances between neighbours are comparatively high, they discard noise terms with high frequencies such as the one shown in the previous example.
In certain cases, like the one presented in this section, this will lead to better function approximations for neighbour approximation methods compared to alternatives such as given gradients and finite differences, as we will show later on.

\section{Other types of transformations}

Until now, the focus was primarily on deterministic linear transformations, because they are relatively easy to construct using active subspaces.
However, in theory we can use arbitrary transformation functions.

\subsection{Random linear transformations}

A completely different approach to finding a transformation function is just generating a random one.
A random transformation generator is easy to implement, can generate new transformation functions almost instantly and offers a purely exploratory approach to finding the best transformation.
It can also serve as a reference for comparision as one would expect that the Active Subspace method should always output better transformation.

Generating uniformly distributed orthonormal matrices is not trivial, but important when generating many of them to avoid any generation bias.
\cite{ABC} presents an algorithm to generate unitary matrices and also a slightly modified version to uniformly generate elements from the orthogonal group $O(d)$.
It starts off with generating a random matrix $Z$ with $z_{i,j} \sim \mathcal{N}\left(0, 1\right)$.
The matrix $Z$ is then fed into a $QR$ decomposition with $Z=QR$.
The entries of the diagonal matrix $R$ are then normalized to get
\begin{equation}
\Lambda=\text{diag}\left(\frac{r_{1,1}}{|r_{1,1}|}, \dots, \frac{r_{d,d}}{|r_{d,d}|}\right)
\end{equation}
The matrix $W=Q \Lambda Q$ is then uniformly distributed with Haar measure, i.e. exactly what we are looking for.



\subsection{Periodic transformations}

Another use case for transformations are periodic functions.
If the model function $f$ is for example periodic along one dimension $i$, i.e. $f(x_1,\dots,x_i,\dots,x_d)=f(x_1,\dots,x_i + p_i, \dots,x_d)$, we can easiliy define a transformation with $t(x_1,\dots,x_i,\dots,x_d)=(x_1,\dots,(x_i\text{ mod } p_i) /p_i, \dots,x_d)^T$ and create a surrogate with $f(x) \approx \hat{f}(t(x))$.
This way, the function can be interpolated better because there are effectively $1 / h_i$ times more grid points spent along the $i$-th dimension.
Of course this can concept can be expanded upon with multiple periodic dimensions, antiperiodic functions, offsets and combination with linear transformations.

A simple example would be the periodic function $f_p(x)=\sin(16\pi x_1)  + \cos(16\pi x_2)$, which repeats in intervals of $\frac{1}{8}$ along both directions.
This can be exploited by constructing a periodic transformation with $p=p_1=p_2=\frac{1}{8}$ as shown above with $t(x_1,x_2)=(\frac{1}{8} (x_1 \text{ mod } \frac{1}{8}), \frac{1}{8} (x_2 \text{ mod } \frac{1}{8}))^T$.
To illustrate the improvement in multiple smaller steps, we also investigate periodic transformations with several configurations $p \in \{\frac{1}{8}, \frac{1}{4}, \frac{1}{2}, 1\}$.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{graphics/periodic}
\captionof{figure}{}
\label{fig:periodic}
\end{figure}

\subsection{Active Manifolds}

Inspired from Active Subspaces, which are purely linear in their nature, one can extend the concept to identifying a one-dimensional curve in the domain $\Omega$ along the flow of most active change, which is called the Active Manifold.
Compared to Active Subspaces, calculating the Active Manifold is more complex and resource intensive.
Furthermore, defining a transformation function that maps from $\Omega$ onto the local one-dimensional space of the curve is way more complex and can be constructed in many ways.
Also they are currently limited to one dimension, for which the surrogate construction method becomes pretty irrelevant.

\section{Conclusion}

This chapter started out by showing how the orientation of function terms affects surrogates created in the standard way and gave a motivation on why using input transformations may be beneficial.
We then showed how, given an orthonormal alignment matrix $W$ and a $r \leq d$, we can construct a transformation function that maps between hypercubes from $\Omega$ to $\Omega_r$ and preserves function structures that can be exploited by the lower dimensional surrogate.
The main focus lied on the active subspaces method to determine $W$, and we extensively looked at different methods to compute the active subspace input matrix $C$.
We then investigated the quality of the resulting active subspaces $W$ by comparing them to the true active subspace $W*$, and observed that even though the approximated gradients are not accurate at all, the resulting active subspaces are still decent and usable.

Now that we are able to create a transformation function $t(x)$

\chapter{Transformed surrogates}
\label{chap:c4}

Once a transformation function $t(x)$ has been created, the next step is creating a surrogate in the reduced input space $\Omega_r$.
As we will see in this chapter, this is not trivial to do with sparse grids and requires special care as there are several challenges that come with the usage of input transformations.
Some of these are specific to sparse grids while others are independent of the used surrogate type.


\section{Transformed input space}

All transformed inputs form the transformed input space, which is a part of the reduced surrogate space $\Omega_r$.
The transformation function was constructed in the last chapter in such a way that every transformed inputs lies in $\Omega_r$, while also applying scaling to use as much space of $\Omega_r$ as possible.

\begin{definition}[Transformed input space]
Let $t \colon \Omega \mapsto \Omega_r$ be transformation function.
We define the transformed input space as
\begin{equation}
T=\{t(x) \mid x \in\Omega\}, ~~ T \subseteq \Omega_r
\end{equation}
\end{definition}

The main difference between $T$ and $\Omega_r$ is that $\Omega_r$ is a unit hypercube, while $T$ might not be one.
As we will show later, in many cases $T$ is not a unit hypercube and it holds that $T \subset \Omega_r$.
We should strive to have $T$ cover $\Omega_r$ as much as possible to not make a portion of $\Omega_r$ go unused when constructing the surrogate.

\begin{definition}[Unusued reduced surrogate inputs]
Let $t \colon \Omega \mapsto \Omega_r$ be transformation function, and let $U_t=T \setminus \Omega_r$ be the unused reduced surrogate space.
Then
\begin{equation}
\tilde{u}_t=\frac{|U_t|}{|\Omega_r|}
\end{equation}
is the share of unused inputs of the reduced surrogate space.
\end{definition}

The greater $U_t$ and therefore the unused input share $\tilde{u}_t$, the more grid points can potentially be generated outside of $T$ if sparse grids are created in a regular way instead of employing adaptivity.
Note that even if a grid point lies outside of $T$, it can still contribute to some degree to the approximation error of the surrogate if the support of the basis function $\phi$ intersects with $T$.
Thus, grid points are only completely unused if their basis function support does not intersect with $T$.
Other surrogate types do not have the same issue, as RBFs for example can freely choose their support points and therefore automatically adapt to $T$.


\section{Method selection}

In the standard approach, we make use of sparse grid interpolation, which is able to generate an accurate interpolant quickly as described in section \ref{}.
However, as we will show in this section, sparse grid interpolation is not suitable for many scenarios with transformed sparse grids and we will therefore differentiate between two cases.
If it holds that $r=d$, i.e. we do not perform a dimension reduction, the transformation can be seen as orientation changing only.
In that case, if a linear transformation is created according to the rules of the previous chapter s.t. all inputs are transformed into a unit hypercube with equal dimension, the transformation function is injective and it maps from $\Omega$ into $T \subseteq \Omega$.
Figure \ref{fig:aligned_grid} illustrates this case.


\begin{mdframed}[style=style]
\begin{figure}[H]
        \centering
\begin{minipage}{.49\textwidth}
        \centering
  \captionof{figure}{Illustration of a possible case with $r=d=2$. The \beige square represents $\Omega=\Omega_r$, the black dots are the grid points of a sparse grid surrogate, and the rotated \lightblue square is $T$. The \grey square in top right corner represents the support of the grid point in its center.}
    \end{minipage}%
    \begin{minipage}{0.49\textwidth}
        \centering
   \includegraphics[width=0.8\linewidth]{graphics/aligned_grid}
\label{fig:aligned_grid}
    \end{minipage}
\end{figure}
\end{mdframed}


It is possible to use normal sparse grid interpolation to create a surrogate for the transformation, if we define how to treat function evaluations for grid points outside of $\Omega$.
For that we can either define a rule on how to treat these function evaluations, or ensure that grid points outside of $T$ never are created.
Restricting grid point creation in $U_t$ is difficult as the grid point hierarchy, which dictates that every grid point has a parent, should be kept intact to work effectively with a sparse grid.
Thus, restricting grid point creation in $U_t$ can prevent large areas in $\Omega$ not being covered by grid points as a basic parent grid point might lie outside of $T$.
Note that even if there are many grid points that were generated outside of $T$, as seen in figure \ref{}, no grid point is actually completely unused, as the support of every grid point basis function intersects at least barely with $T$ and contributes to the approximation error.

The alternative approach would first require to create a modified model function $f' \colon \mathds{R}^d \mapsto \mathds{R}$ that can be evaluated outside of $\Omega$.
We can either set all values outside $\Omega$ to 0 with
\begin{equation}
f'_0(x)=\begin{cases}
f(x)&, \text{if $x \in \Omega$}\\
0&,\text{else}
\end{cases}
\end{equation}
or evaluate the model function outside of $\Omega$ with $f'_{\text{ext}}(x)=f(x)$.
As the transformation function is an automorphism of $\Omega$ when $r=d$, it is invertible for elements $t \in T$ and the inverse function $t^{-1} \colon T \mapsto \Omega$ can easily be constructed as $t$ is linear.
We can extend the inverse transformation $t^{-1}$ to accept all elements $x \in \Omega$ without a problem with $t^{-1}_{\text{ext}} \colon \Omega \mapsto \mathds{R}$ and use it to create the function $\tilde{f}(x)=f'(t^{-1}_{\text{ext}}(x))$ that can be used for sparse grid interpolant creation.

However, if we perform a dimension reduction with $r < d$, we can no longer effectively make use of sparse grid interpolation as the transformation is longer invertible, as many inputs can be projected onto one point with $t(x_1)=\dots=t(x_n)$.
We are then dealing with data that might be, depending on the model function and the transformation, very noisy and there also might be multiple different values for the same inputs, with
\begin{equation}
t(x_i)=t(x_j), f(x_i) \neq f(x_j), ~ x_i \neq x_j
\end{equation}
As a result, the transformed surrogate construction is usually an ill-posed problem for $r < d$, and we have to employ sparse grid based regression methods to successfully create a surrogate for the transformed data.

\begin{mdframed}[style=style]
\begin{figure}[H]
        \centering
\begin{minipage}{.49\textwidth}
        \centering
  \captionof{figure}{Illustration of a possible case with $r=1$ and $d=2$, where the process in broken down into two steps. The surrounding \beige square represents $\Omega$, the \red line represents $\Omega_r$, the black dots are the grid points of a sparse grid surrogate, and the rotated \lightblue square is $T$.
The \green points are sample points that are being mapped from $\Omega$ onto $\Omega_r$.
Note that as $\Omega$ and $\Omega_r$ are distinct vector spaces, this figure visualizes the $\Omega_r$ as the elements crossing the center with $\Omega_r \times \{\frac{1}{2}\}$ in the same way as \cref{sec:vis_trans}.}
    \end{minipage}%
    \begin{minipage}{0.49\textwidth}
        \centering
   \includegraphics[width=0.8\linewidth]{graphics/reduced_grid}
\label{fig:reduced_grid}
    \end{minipage}
\end{figure}
\end{mdframed}

The interpolation based approach will however not be used or investigated in this thesis, as the focus lies on cases with $r < d$ and the regression approach also being able to handle the case with $r=d$.
It is however a certainly interesting approach worthy of being investigated further mainly with the goal of improving the approximation errors for sparse grid interpolants even further.

\section{Sparse grid regression}

The original function samples
\begin{equation}
S=\{(x_1, f(x_1), \dots, (x_n, f(x_n))\} \subseteq \Omega \times \mathds{R}
\end{equation}
that were generated randomly according to the given input probability distribution $\rho$, are transformed to obtain the transformed sample
\begin{equation}
S_t=\{(t(x_1), f(x_1), \dots, (t(x_n), f(x_n))\} \subseteq T \times \mathds{R}
\end{equation}
We are aiming to construct the surrogate function $\hat{f}_t$ using sparse grids with the sparse grid point set $X$ and basis functions $\phi_{\underline{l},\underline{i}}$ for the transformed sample $S_t$ with
\begin{equation}
\hat{f}_t(x) \coloneqq \sum_{x_{\underline{l},\underline{i}} \in X} \alpha_{\underline{l},\underline{i}} \phi_{\underline{l},\underline{i}}(x)
\end{equation}

The basic approach would be to solve the least squares problem with the standard square loss function
\begin{equation}
\epsilon(\hat{f}_t)=\frac{1}{n} \sum_{i=1}^n (f(x_i) - \hat{f}_r(t(x_i)))^2 
\end{equation}

Since the surrogate construction for a transformation is usually an ill-posed problem, might contain random noise, and we are calculating a regressed function only based on the transformed samples $S_t$, the regression method used employs regularization as well to prevent overfitting.
We are therefore solving the regularized least squares problem with the smoothing factor $\lambda > 0$:
\begin{equation}
\hat{f}_t^{\lambda^*} = \argmin_{\hat{f}_r^\lambda \in V} \epsilon(\hat{f}_r^\lambda) + \lambda R(\hat{f}_r^\lambda)
\end{equation}

Compared to sparse grid interpolation, the regression method takes longer to construct a well fitting surrogate as an optimization problem that involves all grid points has to be solved.
The regression algorithm also provides the ability to make use of spatially adaptivity to refine the grid in areas where the difference between surrogate value and training data $\hat{f}_t(x_i) - f(x_i)$ is large.


\section{Multifidelity surrogates}

The construction of the best possible Sparse Grid surrogate, given inputs, a transformation, and set of possible configuration parameters can become very computationally expensive, since every parameter combination has to be evaluated.
To mitigate this problem, we employ the concept of multifidelity simulations \cite{}, i.e. reducing the cost of parametrization by using low fidelity Sparse Grids and datasets to evaluate a parameter combination and only using high fidelity Sparse Grids and datasets when creating the final surrogate using the determined to be optimal parameters.

A low fidelity Sparse Grid surrogate $\hat{f}^l$ has less grid points compared to an otherwise identical high fidelity Sparse Grid surrogate $\hat{f}^h$.
This means that the level of $\hat{f}^l$ is lower and less or not refinements at all are used.
It is vital that the used low fidelity surrogates still stay representative when comparing the errors for different parameter combinations.
They don't have to be representative of the high fidelity surrogate error, but should fullfil the following property:
\begin{equation}
\epsilon(\hat{f}_1^l) < \epsilon(\hat{f}_2^l) \Rightarrow \epsilon(\hat{f}_1^h) < \epsilon(\hat{f}_2^h)
\end{equation}
i.e. they should be indicative of the comparative quality of their high fidelity surrogates.

This multifidelity approach will also be used in later chapters in a more extended fashion to also evaluate multiple different transformations to choose the best one.
As already mentioned, the ability to evaluate transformations is critical for the random transformation method \ref{}, as it allows for the generation and evaluation of many more transformation to choose the best from.

\section{Operations on transformed surrogates}

Now that a surrogate has been created and we can approximate the model function with $f(x) \approx \hat{f}_r(t(x))$,
we can look at the differences when using transformed surrogates compared to the standard case of directly approximating the model function with $f(x) \approx \hat{f}(x)$ and not using a transformation.

\textbf{Differentiation }
Assuming that the surrogate uses a basis that can be differentiated easily, such as B-Splines, the gradient can be approximated using the chain rule and the gradient function of the transformed surrogate with
\begin{equation}
\nabla f(x) \approx \nabla \hat{f}_t(t(x)) = \nabla (\hat{f}_t \circ t)(x)=(Dt(x))^T \nabla \hat{f}_t(t(x))
\end{equation}
\\
\\
\textbf{Inverting the transformation  }
For certain operations it is necessary to define the inverse of the transformation function.
For this we define the inverse transformation as follows:
\begin{equation}
t^{\text{inv}}(x_{t})=\{x \in \Omega \mid t(x)=x_{t}\}
\end{equation}
In the case of a linear dimensionality preserving transformation, it holds that $|t^{\text{inv}}(x_{t})| \in \{0,1\}$.
Otherwise, we can't infer anything about the cardinality of the inverse, i.e. $|t^{\text{inv}}(x_{t})| \geq 0$.
Luckily there is no need to compute $t^{\text{inv}}(x_{t})$. Instead for conducting operations on the surrogate, it suffices to determine wether a point $x_t \in \Omega_t$ is unused, i.e. $|t^{\text{inv}}(x_{t})| = 0$, which can be done quickly.
\vspace{1em}
\textbf{Transformed distribution}
The input probability distribution $\rho \colon \Omega \to \mathds{R_+}$ with $\int_{\Omega} \rho \; \text{d}x = 1$ also changes for the transformation surrogate.
We define the transformed distribution as
\begin{equation}
\rho_r \colon \Omega_r \to \mathds{R_+}, ~~ \rho_r(x_r)=\int_{t^{\text{inv}}(x_r)} \rho(x) \; \text{d}x 
\end{equation}
This is also a distribution since it holds that
\begin{equation}
\int_{\Omega_r} \rho_r(x_t) \; \text{d}x=\int_{x_r \in \Omega_r} \int_{t^{\text{inv}}(x_r)} \rho(x) \; \text{d}x = \int_{\Omega} \rho(x) \; \text{d}x = 1
\end{equation}
This means that in many cases for example a uniform distribution $\rho$ will be transformed into a non-uniform distribution $\rho_r$.
\\
\\
\textbf{Density Quadrature}
Given an input distribution $\rho$ and transformation $t$, we can compute the integral with
\begin{equation}
\int_{\Omega} f(x) \rho(x) \; \text{d}x \approx \int_{\Omega} \hat{f}_t(t(x)) \rho(x) \; \text{d}x
=
\int_{\Omega_r} \hat{f}_t(z) \left(\int_{t^{\text{inv}}(z)} \rho(x)  \; \text{d}x \right)  \; \text{d}z=
\int_{\Omega_r} \hat{f}_t(z) \rho_t(z) \; \text{d}z
\end{equation}
By introducing a transformation we lose the ability to easily use Sparse Grid based quadrature algorithms.
However, since $r$ is usually smaller than $d$, Monte-Carlo based quadrature becomes a better option for transformed surrogates.
\\
\\
\textbf{Quadrature}
Given a transformation $t$, we can easily compute $\int_{\Omega} f(x) \; \text{d}x$ using the surrogate, since it is a special case of density quadrature with $\rho=\mathcal{U}(0,1)^d$.
Even though the original input is uniformly distributed, the resulting surrogate distribution $\rho_r$ is usually not.
\\
\\
\textbf{Optimization}
To calculate the maximum $x^\text{max} \coloneqq \argmax_{x \in \Omega} f(x)$ or minimum $x^\text{min} \coloneqq \argmin_{x \in \Omega} f(x)$ of the model function, one can also optimize the surrogate. There exist many different algorithms, especially for B-Spline basis functions presented in \cite{}.
After having applied a transformation, there might unused points $x_r \in \Omega_r$ in the surrogate space, i.e. $\nexists x \in \Omega \colon t(x)=x_r$.
Therefore a constrained optimization has to be performed on the surrogate first with
\begin{equation}
x_{r}^\text{max} \coloneqq \argmax_{x_r \in \Omega_r} \hat{f}(x_r), ~ |t^{\text{inv}}(x_{r})|\geq 1
\end{equation}
Afterwards, at least in the dimensionality preserving transformation, we can obtain the maximum $x^\text{max}$ with $x^\text{max}=t^{\text{inv}}(x_{r}^\text{max})$, since $|t^{\text{inv}}(x_{r}^\text{max})|=1$.
One problem is that in the case of a reducing transformation with $r<d$, a computed surrogate maximum $x_{r}^\text{max}$ may be a set of many points.
Therefore another non sparse grid based optimization can be performed on the set $t^{\text{inv}}(x_{r}^\text{max})$, to get a good maximum estimate.

\chapter{Algorithm}
\label{chap:c5}

We now covered all the required steps to approximate the model function with a transformed surrogate.
However, we can easily see that using just one transformed surrogate for approximation is not as powerful as it needs to be for many models.
In many cases, a multistep approach that generates a sum of transformed surrogates suits a model function better than a single one, especially if the model function is a sum of function terms itself.
For example, having multiple surrogates allows us to approximate model functions that consist out of multiple terms with a low intrinsic dimension, such as $f(x_1, x_2)=\sin(2 \pi x_1) + x_2$, which has two terms with an intrinsic dimension of one.

\section{Projection pursuit regression}

We take a look at similar methods and take some inspirations from them.
One of these related methods is projection pursuit regression \cite{}.
The projection pursuit regression method comes from the area of statistics and is in its core idea related to our approach with linear transformations.
It tries to represent a dataset
\begin{equation}
P=\{(x_i, y_i)\}=\{(x_i, f(x_i))\}
\end{equation}
using the form
\begin{equation}
y_i=\beta_0 + \sum_{k=1}^m f_k(\beta_k^T x_i) + \epsilon_i
\label{ppr}
\end{equation}
where $\beta_0$ is a constant, $\beta_k$ are projection matrices, the $f_k$ are lower dimensional function surrogates, and the $\epsilon_i$ are the residiuals.
Using the $\beta_k$ to project inputs onto a lower-dimensional space is very similar to our concept of creating linear transformations.
The functions $f_k$ and projection matrices $\beta_k$ of this additive model can then be calculated iteratively by applying one step of finding a projection and constructing a surrogate repeatedly each time on the error function
\begin{equation}
e_k(x)=f(x) - \sum_{i=1}^k f_k(\beta_k^T x)
\end{equation}
This sum can be computed using an iterative approach, as seen in algorithm \ref{alg:ppr}.

\newpage

\begin{mdframed}[style=algstyle,frametitle={\textbf{function} \texttt{projectionPursuitRegression}{$(f, k_{\text{max}})$}}]
\normalsize
\vspace{5.5mm}
\begin{algorithmic}[1]

    \State $e_0 = f$
    \For{$k = 1, \dots, k_{\text{max}}$}
    	\State $\beta_k \gets$ \texttt{determineBestProjection}($e_k$)
    	\State $f_k \gets$ \texttt{determineBestSurrogate}($e_k, \beta_k$)
    	\State $e_k \gets e_{k - 1} - f_k$
    \EndFor
    \State $\hat{f}(x) \gets \sum_{f_k}^m f_k(\beta_k^T x)$
    \State \Return{$\hat{f}$}
\end{algorithmic}

\vspace{-1.5mm}
\delimit

	\captionof{algorithm}{Pseudocode of the iterative projection pursuit regression algorithm. Parameters are the function $f$ and the maximum amount of iterations $k_{\text{max}}$.}
	\label{alg:ppr}
\end{mdframed}

Alternatively, one could also introduce an error exit condition that exits the loop if the regression error is small enough with $\epsilon(e_i) < \epsilon_{max}$.

\section{An iterative approach}

Going from this statistical model to our function approximation problem, we can formulate the same concept using the notation used in this thesis by dropping the constant term $\beta_0$ and residuals $\epsilon_i$ from equation \ref{ppr} to get
\begin{equation}
\hat{f}(x)=\sum_{i=1}^m \hat{f_{t_i}}(t_i(x_i))
\end{equation}
where the $t_i$ are linear transformations similar to the projection with $\beta_k^T$ and the $\hat{f_{t_i}}$ are the surrogate equivalents of the $f_k$.
The functions $\hat{f_{t_i}}$ and transformations $t_i$ can then be calculated iteratively by applying one step of finding a projection and constructing a surrogate repeatedly each time on the adapted error function
\begin{equation}
e_k(x)=f(x) - \sum_{i=1}^k \hat{f_{t_i}}(t_i(x))
\end{equation}

In the case that we use a linear transformation function, the algorithm is pretty much identical to the previous one.
However, this algorithm is more generalized to allow for any type of transformation.

\newpage
\begin{mdframed}[style=algstyle,frametitle={\textbf{function} \texttt{transformedSurrogateSum}{$(f, i_{\text{max}})$}}]
\normalsize
\vspace{5.5mm}
\begin{algorithmic}[1]
    \State $e_0 = f$
    \For{$i = 1, \dots,  i_{\text{max}}$}
    	\State $t_i \gets$ generateBestTransformation($f$)
    	\State $\hat{f}_{t_i} \gets$ generateBestSurroate($f, t_i$)
    	\State $e_i \gets e_{i - 1} - \hat{f}_{t_i}$
    \EndFor
    \State $\hat{f} \gets \sum_{\hat{f}_{t_i}} \hat{f}_{t_i} \circ t_i$
    \State \Return{$\hat{f}$}
\end{algorithmic}
\vspace{-1.5mm}
\delimit
	\captionof{algorithm}{Pseudocode of the iterative projection pursuit regression algorithm. Parameters are the function $f$ and the maximum amount of iterations $i_{\text{max}}$.}
	\label{alg:itappr}
\end{mdframed}

\section{Generating the best surrogate}

Surrogates can, depending on their type, have possibly many independent configuration parameters, which influence their end result and therefore the approximation error.
The amount of possible configuration parameter combinations makes finding the best combination difficult as it would require the examination of a lot of combinations.
The main driver of the runtime is therefore the amount of configurations options to test.
Configuration parameters usually include regression parameters like the smoothing factor $\lambda$, ..., and more.
More formally assuming that we have $m$ configuration parameters where each one has a set of possible values $C_k$, the configuration set would be $C=C_1 \times \dots \times C_m$ and the amount of examinations would be $|C|=|C_1| \dots |C_m|$.

The general structure and properties of the current regression problem at hand can change with each new iteration.
Thus, configuration options have to examined in every iteration again, as their might not exist a set of optimal configuration 
parameters for all iterations.

\begin{mdframed}[style=algstyle,frametitle={\textbf{function} \texttt{generateBestSurroate}{$(S_t, C, \epsilon)$}}]
\normalsize
\vspace{5.5mm}
\begin{algorithmic}[1]
    \State $c^\ast \gets \emptyset$
    \State $\epsilon_{min} \gets \infty$
    \For{$c \in C$}
      \State $\hat{f}_c^l$ = generateLofiSurroate(S, c)
    	\State $\epsilon \gets \epsilon(\hat{f}_c^l)$
    	\If{$\epsilon < \epsilon_{min}$}
    	  \State $\epsilon_{min}\gets e$
    	\State $c^\ast \gets c$
    	\EndIf
    \EndFor
    \State \Return{generateHifiSurroate(S, c*)}
\end{algorithmic}
\vspace{-1.5mm}
\delimit
	\captionof{algorithm}{Pseudocode of the surrogate generation algorithm. Parameters are the sample set $S$, the set of possible surrogate configurations $C$, and an error metric $\epsilon$.}
	\label{alg:bestsur}
\end{mdframed}


It determines the optimal configuration $c*$ with
\begin{equation}
c*=\argmin_{c \in C} \epsilon(\hat{f}_c^l)
\end{equation}
This algorithm works fine presuming that the low fidelity surrogates $\hat{f}_c^l$ are representative of the high fidelity one, as already discussed in chapter \ref{}.
It is satisfactory as long as 
\begin{equation}
\epsilon(\hat{f}_{c*}^h)=\argmin_{c \in C} \epsilon(\hat{f}_c^h)
\end{equation}

\todo{pre convergence}

\section{Finding the optimal reduced dimension}

Until now, we assumed that the reduced dimension $r$ was given and did not cover ways on how to determine the optimal reduced dimension $r*$.
It was mentioned in the active subspace sections in the previous chapter that even though the active subspace method provides a suggested criterion to determine the optimal cutoff dimension $r*$, i.e. looking for the largest gap in the sequence of eigenvalues, this criterion was not always optimal and could be influenced by various factors.
Furthermore, alternative transformation creation methods, like the random transformations, can not make use of such a cutoff criterion.

A solution would be to use the exploratory look ahead approach from the previous section to determine the optimal $r*$.

\begin{algorithm}[H]
\normalsize
\begin{algorithmic}
\Function{generateBestTransformation}{S, $\epsilon$}
    \State c*=$\emptyset$
    \State $e_{min}=\infty$
    \For{r = 1,\dots,d}
      \State $\hat{f}_c^l$ = generateLofiSurroate(S, c)
    	\State $e \gets \epsilon(\hat{f}_c^l)$
    	\If{$e < e_{min}$}
    	  \State $e_{min}\gets e$
    	\State c* $\gets$ c
    	\EndIf
    \EndFor
    \State \Return{generateHifiSurroate(S, c*)}
\EndFunction
\end{algorithmic}
	\captionof{algorithm}{Pseudocode of the surrogate generation algorithm. Parameters are the sample set $S$, the set of possible surrogate configurations $C$, and an error metric $\epsilon$}
\end{algorithm}

\newpage
\section{Visualizing the algorithm}

The iterative algorithm can be visualized with a simple two-dimensional function, where the first component contributes the most to the total function value while the second one does not.
\begin{equation}
f(x_1, x_2)=e^{x_1' + 1} + \sin(2 \pi x_2') + 1, ~~ \begin{pmatrix}
    x_1' \\ x_2'
    \\
  \end{pmatrix} = \begin{pmatrix}
    +\cos(0.1 \pi) & -\sin(0.1 \pi)\\
    +\sin(0.1 \pi) & +\cos(0.1 \pi)
    \\
  \end{pmatrix}\begin{pmatrix}
    x_1 \\ x_2
    \\
  \end{pmatrix}
\end{equation}
Also, it is not axis-aligned to further visualize how the algorithm handles non axis-aligned function terms.

To better illustrate the surrogate construction, $r$ is always set to one, regardless of whether an $r=2$ would make sense or not.
Furthermore, the regression surrogate on the right is only plotted as far as it needs to be, i.e. the plot might not cover $[0,1]$ as the amount of transformed samples in the boundary regions is very low.
This also illustrates how the transformed input distribution changes from the original input distribution, which in this example is uniform.
The amount of samples is set to $n=1000$.
For every iteration, we investigate the error function $e_i$ on the left and look at the regression problem to be solved on the right.

\begin{mdframed}[style=style]
\begin{figure}[H]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{graphics/pipeline_current_1}
  \captionof{figure}{Error function $e_0$ and first active subspace direction $v_1$ (blue).}
  \label{fig:as_rn}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{graphics/pipeline_local_1}
  \captionof{figure}{Transformed input samples (blue) and constructed surrogate in \red.}
  \label{fig:as_nn}
\end{subfigure}
\delimit
\captionof{figure}{First iteration of the iterative transformation algorithm.}
\label{fig:pipeline_1}
\end{figure}
\end{mdframed}


\begin{figure}[H]
\begin{subfigure}{.45\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{graphics/pipeline_current_2}
  \caption{Error function $e_1$ and first active subspace direction $v_1$.}
  \label{fig:as_rn}
\end{subfigure}%
\hspace{0.07\textwidth}
\begin{subfigure}{.45\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{graphics/pipeline_local_2}
  \caption{Transformed surrogate input samples and constructed surrogate.}
  \label{fig:as_nn}
\end{subfigure}
\caption{Second iteration of the iterative transformation algorithm.}
\label{fig:pipeline_2}
\end{figure}

As we can see in figures (\ref{fig:pipeline_1}, \ref{fig:pipeline_2}), the algorithm removes a big chunk of the current error function $e_i$ to reduce the approximation error in each step.
The plotted function values go down in range.

However, as evident in figures (\ref{fig:pipeline_1}, \ref{fig:pipeline_2}), the algorithm hits a point where it can no longer effectively reduce the approximation error by adding another one dimensional surrogate, as the transformed input data is mostly noise that can't be regressed.
Figures ... and ... show that the error function is mainly a two dimensional term.


A two dimensional surrogate would be able to 
In theory, an optimal method should be able to approximate $f$ without a resultung in a leftover two dimensional error function.

\chapter{Implementation}
\label{chap:c6}

To make the shown results as transparent as possible, this chapter will focus on how the previsouly described process of transforming Sparse Grids is actually implemented.


\begin{mdframed}[style=style,frametitle={Notation}]
\begin{figure}[H]


\includegraphics[width=\textwidth]{graphics/definitions.pdf}
\vspace{-7.5mm}

\delimit

\vspace{3.5mm}

\begin{description}
\item[Elements] {~ \begin{enumerate}[\indent{}]
\item \textbf{Object}: An object can represent any kind of data, for example a model function, a function sample, a transformation function, a surrogate, and more.
\item \textbf{Function}: A function takes optional inputs in the form of objects, performs some kind of operation, and outputs an object.
\item \textbf{Component}: A component is a also function, where the actual implementation can be exchanged for many different function implementations of the same interface.
\item \textbf{Control flow + data flow}: Arrows signal control flow and also data flow if an object is shown.
\item \textbf{Gateway}: A gateway evaluates an attached condition and decides which outgoing control flow to follow.
\end{enumerate}}
\end{description}

\delimit

\captionof{figure}{Notation used for different elements in the component structure diagrams in this chapter.}
\label{fig:defs}
\end{figure}
\end{mdframed}


\newpage
\section{Transformation pipeline}
\label{sec:tp}

The complete transformation process is modeled and implemented as an iterative pipeline that is made up of different components to allow for maximum flexibility.
Every pipeline iteration has the goal of adding another transformed surrogate to the current sum of surrogates to further improve the approximation.
Many pipeline components can be exchanged for many different implementation types, where instances of these types can also be customized by passing configuration parameters.
The pipeline structure is visualized in figure \ref{fig:tp}.

\begin{mdframed}[style=style,frametitle={Transformation Pipeline}]
\begin{figure}[H]

\includegraphics[width=\textwidth]{graphics/TransformationPipeline.pdf}
\vspace{-7.5mm}

\delimit

\begin{description}
\item[Parameters] {~ \begin{enumerate}[\indent{}]
\item \texttt{\textbf{exitCondition}}: A predicate indicating whether the pipeline iteration loop should exit. This can be using a variety of conditions such as a maximum iteration count or a maximum approximation error.
\item \texttt{\textbf{sampleCount}}: The amount of function samples $n$ used during pipeline execution. These samples are used for a variety of operations and split into train, validate, and test datasets in each iteration.
\end{enumerate}}
\end{description}

\delimit

\captionof{figure}{Component structure of the transformation pipeline and its associated possible configuration parameters.}
\label{fig:tp}
\end{figure}
\end{mdframed}

The most important component of the transformation pipeline wrt. achieving good approximation results is the transformation generator \ref{sec:tg}, as the quality of a transformation will be passed down the line in each iteration.
The generated transformation is then fed into a surrogate generator \ref{sec:sg}, which will generate the transformed surrogate.
Note that in the abstract pipeline, any type of surrogate like sparse grids and RBFs can be generated in theory.
The newly generated surrogate is then added to the sum of previously generated surrogates to obtain the new transformed surrogate sum.
At the end of each iteration, we decide whether to exit the pipeline or continue.


\newpage
\section {Transformation generators}
\label{sec:tg}

The transformation generator component has the responsibility of finding a good transformation function to use for a pipeline iteration.
We covered the creation of input transformation extensively in chapter \ref{chap:c4}, and transformation generators are a straightforward implementation of the covered methods, with the goal to provide a uniform interface.

\subsection {Transformation stream generator}

We saw previsouly that we can generate a whole family of transformations for every calculated active subspace matrix or random orthogonal basis, which is usually accomplished by using different values for the reduced dimension $r$, i.e. cutting less or more dimensions off.
To apply this concept and to couple it with other components introduced later on, such as transformation evaluators, this implementation makes use of so-called transformation stream generators.

\begin{mdframed}[style=style,frametitle={Transformation generator (stream-based)}]
\begin{figure}[H]
\includegraphics[width=\textwidth]{graphics/TransformationGen_Stream.pdf}
\delimit

\captionof{figure}{Component structure of a stream-based transformation generator.}
\end{figure}
\end{mdframed}

A transformation stream generator will generate as much different transformations as it can, where the range of possible transformations is defined by multiple parameters.
Every transformation that is generated from the stream is evaluated using a transformation evaluator and the best transformation will be chosen and returned in the end.

\newpage

\subsection{Active Subpsace stream generator}

Transformation generation with the active subspace method as covered in \ref{sec:as} is implemented as a transformation stream generator componen, since there are multiple different cutoff possibilities with $r$.
The exact range of the cutoff dimensions can be customized using various parameters as seen in figure \ref{fig:astsg}.

\begin{mdframed}[style=style,frametitle={Transformation stream generator (active subspaces)}]
\begin{figure}[H]
\vspace{5px}
\includegraphics[width=\textwidth]{graphics/TransformationStreamGen_AS.pdf}

\delimit

\begin{description}
\item[Parameters] {~ \begin{enumerate}[\indent{}]
\item \texttt{\textbf{maxSampleCount}}: The maximum amount of samples used to create a gradient sample.
\item \texttt{\textbf{minDimensions}}: The minimum amount of used eigenvectors, i.e. the lower bound for $r$.
\item \texttt{\textbf{maxDimensions}}: The maximum amount of used eigenvectors, i.e. the upper bound for $r$.
\item \texttt{\textbf{minEigenValueShare (= 0)}}: Imposes a lower bound on the possible values of $r$ by requiring that it holds that $\sum_{i=1}^r \lambda_i / \sum_{i=1}^d \lambda_i \geq \texttt{minEigenValueShare}$.
\end{enumerate}}
\end{description}
\delimit
\captionof{figure}{Component structure of an active subspace transformation stream generator and the associated possible configuration parameters.}
\label{fig:astsg}
\end{figure}
\end{mdframed}

Setting a \texttt{minEigenValueShare} value allows us to flexibly adjust the upper bound of $r$ such that we do not consider the last few eigenvectors with way smaller eigenvalues compared to the first few.
A common values used is $\texttt{minEigenValueShare=0.95}$.

The Monte-Carlo based active subspace method requires gradient samples as inputs.
These are generated by a gradient sample generator component.
Possible implementations for gradient computation that we already covered in section \ref{} are available to choose from, as seen in figure \ref{fig:gg}.


\newpage

\begin{mdframed}[style=style,frametitle={Gradient sample generator}]
\begin{figure}[H]
\begin{description}
\item[\texttt{\textbf{givenGradient()}}:] If the gradient function of the model function $f$ is known or easy to calculate analytically, then generating a gradient sample is straightforward by just evaluating the gradient function at the sample points.
\item[\texttt{\textbf{finiteDifferences($h$)}}:] Alternatively, the sample gradients can be determined using finite differences if the runtime cost of $d$ model function evaluations per sample point is acceptable.
\item[\texttt{\textbf{randomNeighbour($n'$)}}] Random neighbour approximation according to \ref{}.
\item[\texttt{\textbf{nearestNeighbour($n', m$)}}] Nearest neighbour approximation according to \ref{}.
\end{description}
\delimit
\captionof{figure}{List of available gradient sample generator implementations.}
\label{fig:gg}
\end{figure}
\end{mdframed}

\subsection {Random stream generator}

An alternative approach to finding a transformation function is just generating a random orthonormal basis, as covered in \ref{}.
This random transformation generator is easy to implement, as shown in figure \ref{rtsg}, can generate new transformation functions almost instantly and offers a purely exploratory approach to finding the best transformation.

\begin{mdframed}[style=style,frametitle={Transformation stream generator (random)}]
\begin{figure}[H]

\vspace{5px}
\includegraphics[width=\textwidth]{graphics/TransformationStreamGen_Random.pdf}

\delimit
\begin{description}
\item[Parameters] {~ \begin{enumerate}[\indent{}]
\item \texttt{\textbf{minDimensions}}: The minimum amount of used basis vectors, i.e. the lower bound for $r$.
\item \texttt{\textbf{maxDimensions}}: The maximum amount of used basis vectors, i.e. the upper bound for $r$.
\end{enumerate}}
\end{description}
\delimit
\captionof{figure}{Component structure of an random transformation stream generator and the associated possible configuration paremeters.}
\label{fig:rtsg}
\end{figure}
\end{mdframed}

\newpage

\subsection {Iterative transformation generator}

In some cases, especially when dealing with a random transformation generation as shown in \ref{sec:rtg}, the probability of finding a good transformation in one try is very low, it makes sense to also introduce an iterative transformation generator component, which can be combined with any other transformation generator component, e.g. a stream-based one.
It can also be used with the active subspace transformation generator to try to find the best transformation out of several active subspace calculations with different input samples.

\begin{mdframed}[style=style,frametitle={Transformation generator (iterative)}]
\begin{figure}[H]
	\includegraphics[width=\textwidth]{graphics/TransformationGen_Iterative.pdf}

\delimit

\begin{description}
\item[Parameters] {~ \begin{enumerate}[\indent{}]
\item \texttt{\textbf{numIterations}}: The amount of iterations that should be executed.
\end{enumerate}}
\end{description}

\delimit
\captionof{figure}{Component structure of an iterative transformation generator and the associated possible configuration parameters.}
\label{fig:itg}
\end{figure}
\end{mdframed}


Note that this iterative approach is independent from the pipeline iterations, i.e. an iterative transformation generator executes all of its iterations during one pipeline step to determine the best transformation to be used in the one pipeline step.

\todo{Lofi}

\newpage
\section {Evaluators}

\subsection {Surrogate evaluator}

The role of a transformation evaluator is to take a newly constructed transformation $t(x)$, evaluate its quality and make it comparable to other transformations.
This is used for iterative and stream-based transformation generators and by the pipeline.
While this is a pretty straightforward process, it can still be customized by changing the surrogate construction rules and error metric.

\begin{mdframed}[style=style,frametitle={Surrogate evaluator}]
\begin{figure}[H]
\includegraphics[width=\textwidth]{graphics/SurrogateEval.pdf}
\vspace{-4.5mm}

\delimit

\begin{description}
\item[Parameters] {~ \begin{enumerate}[\indent{}]
\item \texttt{\textbf{errorMetric}}: The amount of iterations that should be executed.
\end{enumerate}}
\end{description}

\delimit

\captionof{figure}{Component structure of a surrogate evaluator and the associated possible configuration parameters.}
\label{fig:se}
\end{figure}
\end{mdframed}

Common error metrics used in this thesis include MSE, RMSE, and NRMSE.
If we want to steer the transformation generation process into a certain direction, like preferring lower-dimensional transformations, we can introduce a few modifications, such as a penalty term that grows with the amount of reduced dimensions of the transformation.

\subsection {Transformation evaluator}

The role of a transformation evaluator is to take a newly constructed transformation and evaluate its quality by assigning it a certain value to make it comparable to other transformations.
As mentioned in section \ref{}, evaluating the true quality of a transformation is only really possible by creating a surrogate from it.
We therefore first generate a surrogate with a surrogate generator \ref{} and then make use of a surrogate evaluator \ref{} to return the evaluation result.
While this is a pretty straightforward process, it can still be customized by changing the surrogate construction rules, for example by using a lofi \ref{} surrogate generator to speed up the evaluation.

\newpage
\begin{mdframed}[style=style,frametitle={Transformation evaluator (forward looking)}]
\begin{figure}[H]
\includegraphics[width=\textwidth]{graphics/TransformationEval.pdf}
\delimit

\captionof{figure}{Component structure of a forward looking transformation evaluator.}
\label{fig:te}
\end{figure}
\end{mdframed}

\section {Surrogate generator}

The last component needed for the pipeline implementation are surrogate generators, which construct the function $\hat{f}_t$ for given configuration parameters and the transformed input samples
\begin{equation}
\{(t(x_1), f(x_1), \dots, (t(x_n), f(x_n))\} \subseteq T \times \mathds{R}
\end{equation}
Their implementation is straightforward, as the construction of sparse grid surrogates and also RBF surrogates was covered in detail and the surrogate generators closely match that.


\subsection {Sparse Grid surrogate generator}

In the case of Sparse Grid surrogates, as used in this thesis, the construction parameters contain all the normally required inputs like grid type, basis type, adaptivity properties, and regression parameters.
Some sparse grid parameters, like the level, also have to be adapted to handle arbitrary surrogate dimension, as a surrogate generator should be decoupled from the used dimension $r$. 
This means that instead of passing a fixed level $l$ as a parameter, we instead use \texttt{approxGridPoints} to specify how much grid points the surrogate should approximately have and determine the approapriate level from there as it varies greatly for different values of $r$.

\newpage

\begin{mdframed}[style=style,frametitle={Surrogate generator (sparse grid)}]
\begin{figure}[H]
\begin{description}
\item[Parameters] {~ \begin{enumerate}[\indent{}]
\item \texttt{\textbf{gridType}}: Type of basis functions according to \ref{}
\item \texttt{\textbf{basisType}}: Type of basis functions according to \ref{}
\item \texttt{\textbf{numApproxGridPoints}}: Type of basis functions according to \ref{}
\item \texttt{\textbf{numMaxTrainSamples}}: Type of basis functions according to \ref{}
\item \texttt{\textbf{trainData}}: Type of basis functions according to \ref{}
\item \texttt{\textbf{lambdas}}: Type of basis functions according to \ref{}
\item \texttt{\textbf{numRefinements}}: Type of basis functions according to \ref{}
\end{enumerate}}
\end{description}

\delimit

\captionof{figure}{Possible configuration parameters ofa sparse grid surrogate generator.}
\label{fig:sgsg}
\end{figure}
\end{mdframed}

\subsection {RBF surrogate generator}


\section{Function sample budgeting}


\chapter{Evaluation}
\label{chap:c7}

\chapter{Automatic configurations}
\label{chap:c8}

As shown in the previous chapters, there are a lot of configuration options for the complete reduction pipeline.
The goal in this chapter is to provide good default configurations and also an algorithm for automatic configuration generation based on a given model function, using the insights from the previous chapter.


\appendix
\chapter{Appendix}

\subsection{B-Spline basis}

One downside of the hat basis is that the functions do not have a continuous derivative.
Instead, there are discontinuities at the grid point $x_{l,i}$ itself where the derivative flips its sign and at the neighbouring grid points $x_{l,i+2}$ and $x_{l,i-2}$ where the derivative changes to zero.
One solution to eliminating these discontinuities are B-Splines, which are just piecewise polynomials.
\begin{definition}[B-Splines]
Let $p \in \mathds{N}$.
Then
\begin{equation}
b^0(x) \coloneqq
\begin{cases}
    1, & x \in [0,1) \\
   0, & \text{else}
\end{cases}
\end{equation}
is a B-Spline of degree $0$ and
\begin{equation}
b^p(x) \coloneqq \frac{1}{p} xb^{p-1}(x) + (p + 1 - x) b^{p-1}(x-1) 
\end{equation}
is a B-Spline of degree $p$.
\end{definition}
This definition of B-Splines is based on the Cox-de-Boor recursion.

\begin{definition}[B-Spline basis functions]
Let $l,i \in \mathds{N}_0$.
Then
\begin{equation}
\phi^b_{l,i}(x) \coloneqq b^p \left( 2^l x + \frac{p+1}{2} -i \right)
\end{equation}
is an univariate B-Spline basis function of degree $p$.
\end{definition}

\subsection{Mod B-Spline basis}

\begin{definition}[Knots]
Let $m,p \in \mathds{N}_0$.
We then define
\begin{equation}
\underline{\xi}=(\xi_0, \dots, \xi_{m + p}) \in \mathds{R}^{m + p}, ~~ \xi_0 \leq \dots \leq \xi_{m + p}
\end{equation}
as a sequence of knots.
\end{definition}

\begin{definition}[B-Splines]
Let $p \in \mathds{N}$.
Then
\begin{equation}
b^0_{i,\underline{\xi}}(x) \coloneqq
\begin{cases}
    1, & x \in [\xi_i,x_i] \\
   0, & \text{else}
\end{cases}
\end{equation}
is a B-Spline of degree $0$ and
\begin{equation}
b_{i,\underline{\xi}}^p(x) \coloneqq \frac{x - \xi_i}{\xi_{i + p} - \xi_i} b_{i,\underline{\xi}}^{p-1}(x) + \frac{\xi_{i+p+1} - \xi_i}{\xi_{i + p} - \xi_i} b_{i+1,\underline{\xi}}^{p-1}(x) 
\end{equation}
is a B-Spline of degree $p$.
\end{definition}

\begin{definition}[Mod B-Spline basis functions]
Let
\begin{equation}
\phi_l^p(x) \coloneqq \sum_{i=0}^{\lceil (p+1)/2 \rceil} (i+1) \phi^p_{l,1-k}(x)
\end{equation}
be the boundary...
\begin{equation}
\phi^{p,\text{mod}}_{l,i}(x) \coloneqq
\begin{cases}
1 &, l=1\\
\phi^p_{l}(x)&, l>1, i=1\\
\phi^p_{l}(x)&, l>1, i=2^l - 1\\
\phi_{l,i}(x)&, \text{else}
\end{cases}
\nonumber
\end{equation}
\end{definition}


\subsection{Modified basis functions}

One method of removing the need of using boundary grid points at least to some degree are modified basis functions.
These basis functions do not require grid points at the boundary.
Instead at every grid level, they extrapolate the values at the boundaries from the grid points closest to the boundaries using a special kind of basis function.
Therefore, it is possible to modify any non-boundary basis $\phi_{l,i}$ by defining the modified basis as follows:
\begin{equation}
\phi^{\text{mod}}_{l,i}(x) \coloneqq
\begin{cases}
1 &, l=1\\
\phi^{\text{left}}_{l}(x)&, l>1, i=1\\
\phi^{\text{right}}_{l}(x)&, l>1, i=2^l - 1\\
\phi_{l,i}(x)&, \text{else}
\end{cases}
\nonumber
\end{equation}
where $\phi^{\text{left}}_{l}$ and $\phi^{\text{right}}_{l}$ are the special boundary extrapolation functions.
Usually $\phi^{\text{left}}_{l}$ and $\phi^{\text{right}}_{l}$ are similar with regards to their structure of the normal basis functions $\phi_{l,i}$, e.g. a hat basis is usually modified with special hat functions.






\subsection{Prewavelets}

A wavelet is a function that has wavelike properties usually with some kind of oscillation around zero.
Many types of wavelet functions are used for signal processing applications because they inhibit some advantageous properties.
One of these properties is the orthogonality property with $\langle \phi_{i},\phi_{j} \rangle = 0$ for all wavelets with $i \neq j$.
Furthermore,  the wavelets are usually discretisized, because it is not possible to analyze a signal using an infinite amount of wavelets.
One wavelet basis that is already used with sparse grids is the so-called mexican hat basis presented in \cite{}, which does not fullfil any orthogonality property.
In fact, using a completely orthogonal basis is quite constraining, i.e. it is not easy to construct a usable orthogonal basis for the context of sparse grids.
However, in the context of sparse grid based ANOVA we can also work with a semi-orthogonal wavelet basis, also called a prewavelet basis.

\begin{definition}[Semi-Orthogonality]
Let $m,n \in \mathds{N}_0, m \neq n$ be two different levels and $\phi_{l,i}$ a basis.
We then call this basis semi-orthogonal if
\begin{align}
\begin{split}
&\langle \phi_{m,i},\phi_{n,j} \rangle = 0
\nonumber
\end{split}
\end{align}
\end{definition}

The basis functions that are used in this thesis are prewavelets with boundary support \cite{GO95,HP17}, which are linear combinations of the hierarchical hat functions $\phi_{l,i}^h(x)$ and have the desirable property of being semi-orthogonal.

\begin{definition}[Boundary prewavelet basis]
The first basis functions are defined as
\begin{equation}
\phi^p_{0,0} = 1\\
\phi^p_{0, 1} = -\phi_{0,0} + \phi_{0,1}\\
\phi^p_{1, 1} = -\phi_{1,0} + \phi_{1,1} -\phi_{1,2}
\end{equation}
For $l \geq 2$ the basis function are defined as 
\begin{equation}
\phi^p_{l,i} = \frac{1}{10} \phi_{l,i-2} - \frac{6}{10} \phi_{l,i-1} + \frac{10}{10} \phi_{l,i} - \frac{6}{10} \phi_{l,i+1} + \frac{1}{10} \phi_{l,i+2}
\label{prewavelet_def}
\end{equation}
with the special boundary cases
\begin{equation}
\phi^p_{l,1} = -\frac{12}{10} \phi_{l,0} + \frac{11}{10} \phi_{l,1} - \frac{6}{10} \phi_{l,2} + \frac{1}{10} \phi_{l,3}\\ \phi^p_{l,2^l-1}(x)=\phi^p_{l,1}(1-x)
\end{equation}
\end{definition}
The multivariate basis functions $\phi^p_{\underline{l},\underline{i}}$ are obtained by applying the tensor-product approach to the univariate basis functions $\phi^p_{l,i}$ to get $\phi^p_{\underline{l},\underline{i}} \coloneqq \prod_{t=1}^{d} \phi^p_{l_t,i_t}$.
These also fullfil the semi-orthogonality with $\langle \phi^p_{\underline{l},\underline{i}},\phi^p_{\underline{l'},\underline{i'}} \rangle = 0$ for $\underline{l} \neq \underline{l'}$.

Test

\newpage
\printbibliography


\pagestyle{empty}
\renewcommand*{\chapterpagestyle}{empty}
\Versicherung
\end{document}
