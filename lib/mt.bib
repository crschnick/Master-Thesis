% Encoding: UTF-8

@Article{Constantine2014,
  author        = {Paul Constantine and David Gleich},
  title         = {Computing active subspaces with Monte Carlo},
  year          = {2014},
  month         = aug,
  abstract      = {Active subspaces can effectively reduce the dimension of high-dimensional parameter studies enabling otherwise infeasible experiments with expensive simulations. The key components of active subspace methods are the eigenvectors of a symmetric, positive semidefinite matrix whose elements are the average products of partial derivatives of the simulation's input/output map. We study a Monte Carlo method for approximating the eigenpairs of this matrix. We offer both theoretical results based on recent non-asymptotic random matrix theory and a practical approach based on the bootstrap. We extend the analysis to the case when the gradients are approximated, for example, with finite differences. Our goal is to provide guidance for two questions that arise in active subspaces: (i) How many gradient samples does one need to accurately approximate the eigenvalues and subspaces? (ii) What can be said about the accuracy of the estimated subspace, both theoretically and practically? We test the approach on both simple quadratic functions where the active subspace is known and a parameterized PDE with 100 variables characterizing the coefficients of the differential operator.},
  archiveprefix = {arXiv},
  eprint        = {1408.0545},
  file          = {:http\://arxiv.org/pdf/1408.0545v2:PDF},
  keywords      = {math.NA},
  primaryclass  = {math.NA},
}

@Article{Bridges2019,
  author        = {Robert A. Bridges and Anthony D. Gruber and Christopher Felder and Miki Verma and Chelsey Hoff},
  title         = {Active Manifolds: A non-linear analogue to Active Subspaces},
  year          = {2019},
  month         = apr,
  abstract      = {We present an approach to analyze $C^1(\mathbb{R}^m)$ functions that addresses limitations present in the Active Subspaces (AS) method of Constantine et al.(2015; 2014). Under appropriate hypotheses, our Active Manifolds (AM) method identifies a 1-D curve in the domain (the active manifold) on which nearly all values of the unknown function are attained, and which can be exploited for approximation or analysis, especially when $m$ is large (high-dimensional input space). We provide theorems justifying our AM technique and an algorithm permitting functional approximation and sensitivity analysis. Using accessible, low-dimensional functions as initial examples, we show AM reduces approximation error by an order of magnitude compared to AS, at the expense of more computation. Following this, we revisit the sensitivity analysis by Glaws et al. (2017), who apply AS to analyze a magnetohydrodynamic power generator model, and compare the performance of AM on the same data. Our analysis provides detailed information not captured by AS, exhibiting the influence of each parameter individually along an active manifold. Overall, AM represents a novel technique for analyzing functional models with benefits including: reducing $m$-dimensional analysis to a 1-D analogue, permitting more accurate regression than AS (at more computational expense), enabling more informative sensitivity analysis, and granting accessible visualizations(2-D plots) of parameter sensitivity along the AM.},
  archiveprefix = {arXiv},
  eprint        = {1904.13386},
  file          = {:Bridges2019 - Active Manifolds_ a Non Linear Analogue to Active Subspaces.pdf:PDF},
  keywords      = {stat.ML, cs.LG},
  primaryclass  = {stat.ML},
}

@Article{Fukunaga1971,
  author    = {K. Fukunaga and D.R. Olsen},
  journal   = {IEEE Transactions on Computers},
  title     = {An Algorithm for Finding Intrinsic Dimensionality of Data},
  year      = {1971},
  issn      = {2326-3814},
  pages     = {176--183},
  volume    = {C-20},
  abstract  = {An algorithm for the analysis of multivariant data is presented along with some experimental results. The basic idea of the method is to examine the data in many small subregions, and from this determine the number of governing parameters, or intrinsic dimensionality. This intrinsic dimensionality is usually much lower than the dimensionality that is given by the standard Karhunen-LoÃ¨ve technique. An analysis that demonstrates the feasability of this approach is presented.},
  date      = {Feb. 1971},
  doi       = {10.1109/T-C.1971.223208},
  file      = {:Fukunaga1971 - An Algorithm for Finding Intrinsic Dimensionality of Data.html:URL},
  issue     = {2},
  keywords  = {Data reduction, dimensionality reduction, interactive systems, intrinsic dimensionality, Karhunen-LoÃ¨ve expansion, multivariant data analysis, principal component, stochastic processes.},
  publisher = {IEEE},
}
@inproceedings{10.1145/2783258.2783405,
author = {Amsaleg, Laurent and Chelly, Oussama and Furon, Teddy and Girard, St\'{e}phane and Houle, Michael E. and Kawarabayashi, Ken-ichi and Nett, Michael},
title = {Estimating Local Intrinsic Dimensionality},
year = {2015},
isbn = {9781450336642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2783258.2783405},
doi = {10.1145/2783258.2783405},
abstract = {This paper is concerned with the estimation of a local measure of intrinsic dimensionality
(ID) recently proposed by Houle. The local model can be regarded as an extension of
Karger and Ruhl's expansion dimension to a statistical setting in which the distribution
of distances to a query point is modeled in terms of a continuous random variable.
This form of intrinsic dimensionality can be particularly useful in search, classification,
outlier detection, and other contexts in machine learning, databases, and data mining,
as it has been shown to be equivalent to a measure of the discriminative power of
similarity functions. Several estimators of local ID are proposed and analyzed based
on extreme value theory, using maximum likelihood estimation (MLE), the method of
moments (MoM), probability weighted moments (PWM), and regularly varying functions
(RV). An experimental evaluation is also provided, using both real and artificial
data.},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {29–38},
numpages = {10},
keywords = {indiscriminability, manifold learning, intrinsic dimension},
location = {Sydney, NSW, Australia},
series = {KDD '15}
}

@Book{Pflueger2010,
  author    = {Pfl{\"{u}}ger, Dirk},
  publisher = {Verlag Dr. Hut},
  title     = {Spatially Adaptive Sparse Grids for High-Dimensional Problems},
  year      = {2010},
  address   = {M{\"{u}}nchen},
  isbn      = {9783868535556},
  month     = aug,
  school    = {Institut f{\"{u}}r Informatik, Technische Universit{\"{a}}t M{\"{u}}nchen},
  url       = {http://www5.in.tum.de/pub/pflueger10spatially.pdf},
}

@PhdThesis{Feuersaenger2010,
  author = {Christian Feuersänger},
  school = {Mathematisch–Naturwissenschaftliche Fakultät der der Rheinischen Friedrich–Wilhelms–Universität Bonn},
  title  = {Sparse Grid Methods for Higher Dimensional Approximation},
  year   = {2010},
}
@MasterThesis{V14,
  author =	 "Julian Valentin",
  title =	 "Hierarchische Optimierung mit Gradientenverfahren auf Dünngitterfunktionen",
  school =	 "Universität Stuttgart, Institut für Parallele und Verteilte Systeme",
  year =	 "2014",
  url =      {ftp://ftp.informatik.uni-stuttgart.de/pub/library/medoc.ustuttgart_fi/MSTR-3629/MSTR-3629.pdf}
}@INPROCEEDINGS{G13,
    author = {Jochen Garcke},
    title = {Sparse grids in a nutshell},
    booktitle = {Sparse grids and applications},
    year = {2013},
    pages = {57--80},
    publisher = {Springer}
}
@InProceedings{Pflueger2012,
  author    = {Pfl{\"{u}}ger, Dirk},
  booktitle = {Sparse Grids and Applications},
  title     = {Spatially Adaptive Refinement},
  year      = {2012},
  address   = {Berlin Heidelberg},
  editor    = {Garcke, Jochen and Griebel, Michael},
  month     = oct,
  pages     = {243--262},
  publisher = {Springer},
  series    = {Lecture Notes in Computational Science and Engineering},
  url       = {http://www5.in.tum.de/pub/pflueger12spatially_preprint.pdf},
}
@article{S01,
 author = {Sobol\'{a}, I. M.},
 title = {Global Sensitivity Indices for Nonlinear Mathematical Models and Their Monte Carlo Estimates},
 journal = {Math. Comput. Simul.},
 issue_date = {Feb. 15, 2001},
 volume = {55},
 number = {1-3},
 month = feb,
 year = {2001},
 issn = {0378-4754},
 pages = {271--280},
 numpages = {10},
 url = {http://dx.doi.org/10.1016/S0378-4754(00)00270-6},
 doi = {10.1016/S0378-4754(00)00270-6},
 acmid = {373383},
 publisher = {Elsevier Science Publishers B. V.},
 address = {Amsterdam, The Netherlands, The Netherlands},
 keywords = {Monte Carlo method, mathematical modelling, quasi-Monte Carlo method, sensitivity analysis},
} 

@PhdThesis{Valentin2019,
  author        = {Julian Valentin},
  title         = {B-Splines for Sparse Grids: Algorithms and Application to Higher-Dimensional Optimization},
  year          = {2019},
  month         = oct,
  abstract      = {In simulation technology, computationally expensive objective functions are often replaced by cheap surrogates, which can be obtained by interpolation. Full grid interpolation methods suffer from the so-called curse of dimensionality, rendering them infeasible if the parameter domain of the function is higher-dimensional (four or more parameters). Sparse grids constitute a discretization method that drastically eases the curse, while the approximation quality deteriorates only insignificantly. However, conventional basis functions such as piecewise linear functions are not smooth (continuously differentiable). Hence, these basis functions are unsuitable for applications in which gradients are required. One example for such an application is gradient-based optimization, in which the availability of gradients greatly improves the speed of convergence and the accuracy of the results. This thesis demonstrates that hierarchical B-splines on sparse grids are well-suited for obtaining smooth interpolants for higher dimensionalities. The thesis is organized in two main parts: In the first part, we derive new B-spline bases on sparse grids and study their implications on theory and algorithms. In the second part, we consider three real-world applications in optimization: topology optimization, biomechanical continuum-mechanics, and dynamic portfolio choice models in finance. The results reveal that the optimization problems of these applications can be solved accurately and efficiently with hierarchical B-splines on sparse grids.},
  archiveprefix = {arXiv},
  doi           = {10.18419/opus-10504},
  eprint        = {1910.05379},
  file          = {:Valentin2019 - B Splines for Sparse Grids_ Algorithms and Application to Higher Dimensional Optimization.pdf:PDF},
  keywords      = {math.NA, cs.NA, G.1.1; G.1.6; G.4},
  primaryclass  = {math.NA},
}

@Article{Weisstein,
  author  = {Eric W. Weisstein},
  journal = {MathWorld--A Wolfram Web Resource},
  title   = {Hypercube Line Picking.},
  url     = {https://mathworld.wolfram.com/HypercubeLinePicking.html},
}

@Article{Bailey2007,
  author   = {D.H. Bailey and J.M. Borwein and R.E. Crandall},
  journal  = {Journal of Computational and Applied Mathematics},
  title    = {Box integrals},
  year     = {2007},
  issn     = {0377-0427},
  number   = {1},
  pages    = {196-208},
  volume   = {206},
  abstract = {By a “box integral” we mean here an expectation 〈|r⇒-q⇒|s〉 where r⇒ runs over the unit n-cube, with q⇒ and s fixed, explicitly:∫01⋯∫01((r1-q1)2+⋯+(rn-qn)2)s/2dr1…drn.The study of box integrals leads one naturally into several disparate fields of analysis. While previous studies have focused upon symbolic evaluation and asymptotic analysis of special cases (notably s=1), we work herein more generally—in interdisciplinary fashion—developing results such as: (1) analytic continuation (in complex s), (2) relevant combinatorial identities, (3) rapidly converging series, (4) statistical inferences, (5) connections to mathematical physics, and (6) extreme-precision quadrature techniques appropriate for these integrals. These intuitions and results open up avenues of experimental mathematics, with a view to new conjectures and theorems on integrals of this type.},
  doi      = {https://doi.org/10.1016/j.cam.2006.06.010},
  keywords = {Box integrals, Multi-dimensional integrals, High-precision quadrature},
  url      = {https://www.sciencedirect.com/science/article/pii/S0377042706004250},
}
@article{huber1985projection,
  title={Projection pursuit},
  author={Huber, Peter J},
  journal={The annals of Statistics},
  pages={435--475},
  year={1985},
  publisher={JSTOR}
}
@article{Ishigami1990AnIQ,
  title={An importance quantification technique in uncertainty analysis for computer models},
  author={T. Ishigami and T. Homma},
  journal={[1990] Proceedings. First International Symposium on Uncertainty Modeling and Analysis},
  year={1990},
  pages={398-403}
}
@Article{Sobol1999,
  author  = {Sobol, ' and Levitan, I and Y},
  journal = {Computer Physics Communications},
  title   = {On the use of variance reducing multipliers in Monte Carlo computations of a global sensitivity index},
  year    = {1999},
  number  = {1},
  pages   = {52--61},
  volume  = {117},
  date    = {1999},
}
@Inbook{Kress1999,
author="Kress, Rainer",
title="Regularization by Discretization",
bookTitle="Linear Integral Equations",
year="1999",
publisher="Springer New York",
address="New York, NY",
pages="308--319",
abstract="We briefly return to the study of projection methods and will consider their application to ill-posed equations of the first kind. In particular we will present an exposition of the moment discretization method. For further studies of regularization through discretization, we refer to Baumeister [13], Kirsch [87], Louis [116], and Natterer [136].",
isbn="978-1-4612-0559-3",
doi="10.1007/978-1-4612-0559-3_17",
url="https://doi.org/10.1007/978-1-4612-0559-3_17"
}

@Article{Wang2008,
  author   = {Xiaoqun Wang and Ian H. Sloan},
  journal  = {Journal of Computational and Applied Mathematics},
  title    = {Low discrepancy sequences in high dimensions: How well are their projections distributed?},
  year     = {2008},
  issn     = {0377-0427},
  number   = {2},
  pages    = {366-386},
  volume   = {213},
  abstract = {Quasi-Monte Carlo (QMC) methods have been successfully used to compute high-dimensional integrals arising in many applications, especially in finance. To understand the success and the potential limitation of QMC, this paper focuses on quality measures of point sets in high dimensions. We introduce the order-ℓ, superposition and truncation discrepancies, which measure the quality of selected projections of a point set on lower-dimensional spaces. These measures are more informative than the classical ones. We study their relationships with the integration errors and study the tractability issues. We present efficient algorithms to compute these discrepancies and perform computational investigations to compare the performance of the Sobol’ nets with that of the sets of Latin hypercube sampling and random points. Numerical results show that in high dimensions the superiority of the Sobol’ nets mainly derives from the one-dimensional projections and the projections associated with the earlier dimensions; for order-2 and higher-order projections all these point sets have similar behavior (on the average). In weighted cases with fast decaying weights, the Sobol’ nets have a better performance than the other two point sets. The investigation enables us to better understand the properties of QMC and throws new light on when and why QMC can have a better (or no better) performance than Monte Carlo for multivariate integration in high dimensions.},
  doi      = {https://doi.org/10.1016/j.cam.2007.01.005},
  keywords = {Quasi-Monte Carlo methods, Low discrepancy sequences, Discrepancy, Multivariate integration},
  url      = {https://www.sciencedirect.com/science/article/pii/S0377042707000374},
}

@Article{Robinson2006,
  author  = {Robinson, T and Willcox, K and Eldred, Michael and Haimes, Robert},
  journal = {Collection of Technical Papers - 11th AIAA/ISSMO Multidisciplinary Analysis and Optimization Conference},
  title   = {Multifidelity Optimization for Variable-Complexity Design},
  year    = {2006},
  month   = {09},
  volume  = {4},
  doi     = {10.2514/6.2006-7114},
}

@Article{Broomhead1988,
  author  = {Broomhead, David and Lowe, David},
  journal = {Complex Systems},
  title   = {Multivariable Functional Interpolation and Adaptive Networks},
  year    = {1988},
  pages   = {321-355},
  volume  = {2},
  date    = {1988},
}

@InBook{Devroye1986,
  author    = {Devroye, Luc},
  chapter   = {Chapter II.2: The lnverslon method.},
  pages     = {27--40},
  publisher = {Springer-Verlag},
  title     = {Non-Uniform Random Variate Generation},
  year      = {1986},
  address   = {New York},
  date      = {1986},
}

@Misc{Bochkanov,
  author = {Sergey Bochkanov},
  title  = {ALGLIB (www.alglib.net)},
}

@InProceedings{Maleki2011,
  author    = {Maleki, Saeed and Gao, Yaoqing and Garzar´n, Maria J. and Wong, Tommy and Padua, David A.},
  booktitle = {2011 International Conference on Parallel Architectures and Compilation Techniques},
  title     = {An Evaluation of Vectorizing Compilers},
  year      = {2011},
  pages     = {372-382},
  doi       = {10.1109/PACT.2011.68},
}
@misc{ openmp08,
    author = {{OpenMP Architecture Review Board}},
    title = {{OpenMP} Application Program Interface Version 3.0},
    month = may,
    year = 2008,
    url = {http://www.openmp.org/mp-documents/spec30.pdf}
}
@Article{Stone2010,
  author  = {Stone, John E. and Gohara, David and Shi, Guochun},
  journal = {Computing in Science Engineering},
  title   = {OpenCL: A Parallel Programming Standard for Heterogeneous Computing Systems},
  year    = {2010},
  number  = {3},
  pages   = {66-73},
  volume  = {12},
  doi     = {10.1109/MCSE.2010.69},
}

@Article{Crestaux2009,
  author   = {Thierry Crestaux and Olivier {Le Maıˆtre} and Jean-Marc Martinez},
  journal  = {Reliability Engineering \& System Safety},
  title    = {Polynomial chaos expansion for sensitivity analysis},
  year     = {2009},
  issn     = {0951-8320},
  note     = {Special Issue on Sensitivity Analysis},
  number   = {7},
  pages    = {1161-1172},
  volume   = {94},
  abstract = {In this paper, the computation of Sobol's sensitivity indices from the polynomial chaos expansion of a model output involving uncertain inputs is investigated. It is shown that when the model output is smooth with regards to the inputs, a spectral convergence of the computed sensitivity indices is achieved. However, even for smooth outputs the method is limited to a moderate number of inputs, say 10–20, as it becomes computationally too demanding to reach the convergence domain. Alternative methods (such as sampling strategies) are then more attractive. The method is also challenged when the output is non-smooth even when the number of inputs is limited.},
  doi      = {https://doi.org/10.1016/j.ress.2008.10.008},
  keywords = {Sensitivity analysis, Sobol's decomposition, Polynomial chaos, Uncertainty quantification},
  url      = {https://www.sciencedirect.com/science/article/pii/S0951832008002561},
}

@Article{Nobile2008,
  author  = {Nobile, F. and Tempone, R. and Webster, C. G.},
  journal = {SIAM Journal on Numerical Analysis},
  title   = {A Sparse Grid Stochastic Collocation Method for Partial Differential Equations with Random Input Data},
  year    = {2008},
  number  = {5},
  pages   = {2309-2345},
  volume  = {46},
  doi     = {10.1137/060663660},
  eprint  = {https://doi.org/10.1137/060663660},
  url     = {https://doi.org/10.1137/060663660},
}

@Article{Garcke2001,
  author     = {Garcke, J. and Griebel, M. and Thess, M.},
  journal    = {Computing},
  title      = {Data Mining with Sparse Grids},
  year       = {2001},
  issn       = {0010-485X},
  month      = nov,
  number     = {3},
  pages      = {225–253},
  volume     = {67},
  address    = {Berlin, Heidelberg},
  doi        = {10.1007/s006070170007},
  issue_date = {November 2001},
  keywords   = {approximation, classification, combination technique, data mining, sparse grids},
  numpages   = {29},
  publisher  = {Springer-Verlag},
  url        = {https://doi.org/10.1007/s006070170007},
}

@Article{Gerstner1998,
  author  = {Thomas Gerstner and Michael Griebel},
  journal = {NUMER. ALGORITHMS},
  title   = {Numerical Integration using Sparse Grids},
  year    = {1998},
  pages   = {209--232},
  volume  = {18},
}

@PhdThesis{Rehme2021,
  author = {Michael F. Rehme},
  school = {Universität Stuttgart},
  title  = {B-Splines on Sparse Grids forUncertainty Quantification},
  year   = {2021},
}

@Article{Feuersaenger2009,
  author   = {Feuersänger, Christian and Griebel, Michael},
  journal  = {Computing},
  title    = {Principal manifold learning by sparse grids},
  year     = {2009},
  issn     = {1436-5057},
  number   = {4},
  pages    = {267--299},
  volume   = {85},
  abstract = {In this paper, we deal with the construction of lower-dimensional manifolds from high-dimensional data which is an important task in data mining, machine learning and statistics. Here, we consider principal manifolds as the minimum of a regularized, non-linear empirical quantization error functional. For the discretization we use a sparse grid method in latent parameter space. This approach avoids, to some extent, the curse of dimension of conventional grids like in the GTM approach. The arising non-linear problem is solved by a descent method which resembles the expectation maximization algorithm. We present our sparse grid principal manifold approach, discuss its properties and report on the results of numerical experiments for one-, two- and three-dimensional model problems.},
  doi      = {10.1007/s00607-009-0045-8},
  refid    = {Feuersänger2009},
  url      = {https://doi.org/10.1007/s00607-009-0045-8},
}

@Article{Abdi2010,
  author   = {Abdi, Hervé and Williams, Lynne J.},
  journal  = {WIREs Computational Statistics},
  title    = {Principal component analysis},
  year     = {2010},
  number   = {4},
  pages    = {433-459},
  volume   = {2},
  abstract = {Abstract Principal component analysis (PCA) is a multivariate technique that analyzes a data table in which observations are described by several inter-correlated quantitative dependent variables. Its goal is to extract the important information from the table, to represent it as a set of new orthogonal variables called principal components, and to display the pattern of similarity of the observations and of the variables as points in maps. The quality of the PCA model can be evaluated using cross-validation techniques such as the bootstrap and the jackknife. PCA can be generalized as correspondence analysis (CA) in order to handle qualitative variables and as multiple factor analysis (MFA) in order to handle heterogeneous sets of variables. Mathematically, PCA depends upon the eigen-decomposition of positive semi-definite matrices and upon the singular value decomposition (SVD) of rectangular matrices. Copyright © 2010 John Wiley \& Sons, Inc. This article is categorized under: Statistical and Graphical Methods of Data Analysis > Multivariate Analysis Statistical and Graphical Methods of Data Analysis > Dimension Reduction},
  doi      = {https://doi.org/10.1002/wics.101},
  eprint   = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/wics.101},
  keywords = {singular and eigen value decomposition, bilinear decomposition, factor scores and loadings, RESS PRESS, multiple factor analysis},
  url      = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wics.101},
}

@Article{Bennett1969,
  author  = {Bennett, R.},
  journal = {IEEE Transactions on Information Theory},
  title   = {The intrinsic dimensionality of signal collections},
  year    = {1969},
  number  = {5},
  pages   = {517-525},
  volume  = {15},
  doi     = {10.1109/TIT.1969.1054365},
}

@Book{Bellman1961,
  author    = {Richard E. Bellman},
  publisher = {Princeton University Press},
  title     = {Adaptive Control Processes: A Guided Tour},
  year      = {1961},
  isbn      = {9781400874668},
  doi       = {doi:10.1515/9781400874668},
  url       = {https://doi.org/10.1515/9781400874668},
}

@Book{Stetter1973,
  author    = {Stetter, Hans J and others},
  publisher = {Springer},
  title     = {Analysis of discretization methods for ordinary differential equations},
  year      = {1973},
  volume    = {23},
}

@Article{Zenger1991,
  author  = {Zenger, C.},
  journal = {Parallel Algorithms for Partial Differential Equations, Proceedings of the 6th GAMM-Seminar Kiel 1990},
  title   = {Sparse grids},
  year    = {1991},
  month   = jan,
  pages   = {241--251},
}

@InProceedings{Constantine2015,
  author    = {P. Constantine},
  booktitle = {SIAM spotlights},
  title     = {Active Subspaces - Emerging Ideas for Dimension Reduction in Parameter Studies},
  year      = {2015},
}

@PhdThesis{Balder1994,
  author = {Balder, R.},
  school = {Technische Universität München},
  title  = {Adaptive Verfahren für elliptische und parabolische Differentialgleichungen auf dünnen Gittern},
  year   = {1994},
}

@InProceedings{Khakhutskyy2016,
  author    = {Khakhutskyy, Valeriy and Hegland, Markus},
  booktitle = {Sparse Grids and Applications - Stuttgart 2014},
  title     = {Spatially-Dimension-Adaptive Sparse Grids for Online Learning},
  year      = {2016},
  address   = {Cham},
  editor    = {Garcke, Jochen and Pfl{\"u}ger, Dirk},
  pages     = {133--162},
  publisher = {Springer International Publishing},
  abstract  = {This paper takes a new look at regression with adaptive sparse grids. Considering sparse grid refinement as an optimisation problem, we show that it is in fact an instance of submodular optimisation with a cardinality constraint. Hence, we are able to directly apply results obtained in combinatorial optimisation research concerned with submodular optimisation to the grid refinement problem. Based on these results, we derive an efficient refinement indicator that allows the selection of new grid indices with finer granularity than was previously possible. We then implement the resulting new refinement procedure using an averaged stochastic gradient descent method commonly used in online learning methods. As a result we obtain a new method for training adaptive sparse grid models. We show both for synthetic and real-life data that the resulting models exhibit lower complexity and higher predictive power compared to currently used state-of-the-art methods.},
  isbn      = {978-3-319-28262-6},
}

@Article{Niederreiter1988,
  author   = {Harald Niederreiter},
  journal  = {Journal of Number Theory},
  title    = {Low-discrepancy and low-dispersion sequences},
  year     = {1988},
  issn     = {0022-314X},
  number   = {1},
  pages    = {51-70},
  volume   = {30},
  abstract = {We generalize and improve earlier constructions of low-discrepancy sequences by Sobol', Faure, and the author, thus obtaining sequences in the s-dimensional unit cube with the smallest discrepancy that is currently known. The construction is based on the theory of (t, s)-sequences. It is also shown that the dispersion of the sequences constructed here has the smallest possible order of magnitude among any sequences in the s-dimensional unit cube.},
  doi      = {https://doi.org/10.1016/0022-314X(88)90025-X},
  url      = {https://www.sciencedirect.com/science/article/pii/0022314X8890025X},
}

@Book{Forrester2008,
  author = {Forrester, Alexander and Sobester, Andras and Keane, Andy},
  title  = {Engineering Design Via Surrogate Modelling: A Practical Guide},
  year   = {2008},
  isbn   = {978-0-470-06068-1},
  month  = {07},
  doi    = {10.1002/9780470770801},
}

@Article{Moon2012,
  author    = {Moon, Hyejung and Dean, Angela M. and Santner, Thomas J.},
  journal   = {Technometrics},
  title     = {Two-Stage Sensitivity-Based Group Screening in Computer Experiments},
  year      = {2012},
  issn      = {00401706},
  number    = {4},
  pages     = {376--387},
  volume    = {54},
  abstract  = {[Sophisticated computer codes that implement mathematical models of physical processes can involve large numbers of inputs, and screening to determine the most active inputs is critical for understanding the inputoutput relationship. This article presents a new two-stage group screening methodology for identifying active inputs. In Stage 1, groups of inputs showing low activity are screened out; in Stage 2, individual inputs from the active groups are identified. Inputs are evaluated through their estimated total (effect) sensitivity indices (TSIs), which are compared with a benchmark null TSI distribution created from added low noise inputs. Examples show that, compared with other procedures, the proposed method provides more consistent and accurate results for high-dimensional screening. Additional details and computer code are provided in supplementary materials available online.]},
  database  = {JSTOR},
  publisher = {Taylor & Francis, Ltd.},
  url       = {http://www.jstor.org/stable/41714922},
}

@Article{Harper1983,
  author       = {Harper, W V and Gupta, S K},
  title        = {Sensitivity/uncertainty analysis of a borehole scenario comparing Latin Hypercube Sampling and deterministic sensitivity approaches},
  year         = {1983},
  month        = {10},
  abstractnote = {A computer code was used to study steady-state flow for a hypothetical borehole scenario. The model consists of three coupled equations with only eight parameters and three dependent variables. This study focused on steady-state flow as the performance measure of interest. Two different approaches to sensitivity/uncertainty analysis were used on this code. One approach, based on Latin Hypercube Sampling (LHS), is a statistical sampling method, whereas, the second approach is based on the deterministic evaluation of sensitivities. The LHS technique is easy to apply and should work well for codes with a moderate number of parameters. Of deterministic techniques, the direct method is preferred when there are many performance measures of interest and a moderate number of parameters. The adjoint method is recommended when there are a limited number of performance measures and an unlimited number of parameters. This unlimited number of parameters capability can be extremely useful for finite element or finite difference codes with a large number of grid blocks. The Office of Nuclear Waste Isolation will use the technique most appropriate for an individual situation. For example, the adjoint method may be used to reduce the scope to a size that can be readily handled by a technique such as LHS. Other techniques for sensitivity/uncertainty analysis, e.g., kriging followed by conditional simulation, will be used also. 15 references, 4 figures, 9 tables.},
  place        = {United States},
  url          = {https://www.osti.gov/biblio/5355549},
}

@Article{Bungartz2004,
  author  = {Bungartz, Hans-Joachim and Griebel, Michael},
  journal = {In: Acta Numerica. Vol. 13, pp. 147-269},
  title   = {Sparse Grids},
  year    = {2004},
  month   = {05},
  volume  = {13},
  doi     = {10.1017/S0962492904000182},
}

@Article{Huo2008,
  author  = {Huo, Xiaoming and Smith, Andrew},
  journal = {Recent Advances in Data Mining of Enterprise Data},
  title   = {A Survey of Manifold-Based Learning Methods},
  year    = {2008},
  month   = {01},
  doi     = {10.1142/9789812779861_0015},
  isbn    = {978-981-277-985-4},
}

@Article{Dette2010,
  author    = {Holger Dette and Andrey Pepelyshev},
  journal   = {Technometrics},
  title     = {Generalized Latin Hypercube Design for Computer Experiments},
  year      = {2010},
  number    = {4},
  pages     = {421-429},
  volume    = {52},
  doi       = {10.1198/TECH.2010.09157},
  eprint    = {https://doi.org/10.1198/TECH.2010.09157},
  publisher = {Taylor & Francis},
  url       = {https://doi.org/10.1198/TECH.2010.09157},
}

@Article{BertinMahieux2011,
  author = {Bertin-Mahieux, Thierry and Ellis, Daniel PW and Whitman, Brian and Lamere, Paul},
  title  = {The million song dataset},
  year   = {2011},
}

@Article{Altman1992,
  author    = {Altman, Naomi S},
  journal   = {The American Statistician},
  title     = {An introduction to kernel and nearest-neighbor nonparametric regression},
  year      = {1992},
  number    = {3},
  pages     = {175--185},
  volume    = {46},
  publisher = {Taylor \& Francis},
}

@Article{Langford2009,
  author  = {Langford, John and Li, Lihong and Zhang, Tong},
  journal = {Journal of Machine Learning Research},
  title   = {Sparse Online Learning via Truncated Gradient.},
  year    = {2009},
  number  = {3},
  volume  = {10},
}

@Comment{jabref-meta: databaseType:bibtex;}
