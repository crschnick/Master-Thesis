% Encoding: UTF-8

@Article{Constantine2014,
  author        = {Paul Constantine and David Gleich},
  title         = {Computing active subspaces with Monte Carlo},
  year          = {2014},
  month         = aug,
  abstract      = {Active subspaces can effectively reduce the dimension of high-dimensional parameter studies enabling otherwise infeasible experiments with expensive simulations. The key components of active subspace methods are the eigenvectors of a symmetric, positive semidefinite matrix whose elements are the average products of partial derivatives of the simulation's input/output map. We study a Monte Carlo method for approximating the eigenpairs of this matrix. We offer both theoretical results based on recent non-asymptotic random matrix theory and a practical approach based on the bootstrap. We extend the analysis to the case when the gradients are approximated, for example, with finite differences. Our goal is to provide guidance for two questions that arise in active subspaces: (i) How many gradient samples does one need to accurately approximate the eigenvalues and subspaces? (ii) What can be said about the accuracy of the estimated subspace, both theoretically and practically? We test the approach on both simple quadratic functions where the active subspace is known and a parameterized PDE with 100 variables characterizing the coefficients of the differential operator.},
  archiveprefix = {arXiv},
  eprint        = {1408.0545},
  file          = {:http\://arxiv.org/pdf/1408.0545v2:PDF},
  keywords      = {math.NA},
  primaryclass  = {math.NA},
}

@Article{Bridges2019,
  author        = {Robert A. Bridges and Anthony D. Gruber and Christopher Felder and Miki Verma and Chelsey Hoff},
  title         = {Active Manifolds: A non-linear analogue to Active Subspaces},
  year          = {2019},
  month         = apr,
  abstract      = {We present an approach to analyze $C^1(\mathbb{R}^m)$ functions that addresses limitations present in the Active Subspaces (AS) method of Constantine et al.(2015; 2014). Under appropriate hypotheses, our Active Manifolds (AM) method identifies a 1-D curve in the domain (the active manifold) on which nearly all values of the unknown function are attained, and which can be exploited for approximation or analysis, especially when $m$ is large (high-dimensional input space). We provide theorems justifying our AM technique and an algorithm permitting functional approximation and sensitivity analysis. Using accessible, low-dimensional functions as initial examples, we show AM reduces approximation error by an order of magnitude compared to AS, at the expense of more computation. Following this, we revisit the sensitivity analysis by Glaws et al. (2017), who apply AS to analyze a magnetohydrodynamic power generator model, and compare the performance of AM on the same data. Our analysis provides detailed information not captured by AS, exhibiting the influence of each parameter individually along an active manifold. Overall, AM represents a novel technique for analyzing functional models with benefits including: reducing $m$-dimensional analysis to a 1-D analogue, permitting more accurate regression than AS (at more computational expense), enabling more informative sensitivity analysis, and granting accessible visualizations(2-D plots) of parameter sensitivity along the AM.},
  archiveprefix = {arXiv},
  eprint        = {1904.13386},
  file          = {:Bridges2019 - Active Manifolds_ a Non Linear Analogue to Active Subspaces.pdf:PDF},
  keywords      = {stat.ML, cs.LG},
  primaryclass  = {stat.ML},
}

@Article{Fukunaga1971,
  author    = {K. Fukunaga and D.R. Olsen},
  journal   = {IEEE Transactions on Computers},
  title     = {An Algorithm for Finding Intrinsic Dimensionality of Data},
  year      = {1971},
  issn      = {2326-3814},
  pages     = {176--183},
  volume    = {C-20},
  abstract  = {An algorithm for the analysis of multivariant data is presented along with some experimental results. The basic idea of the method is to examine the data in many small subregions, and from this determine the number of governing parameters, or intrinsic dimensionality. This intrinsic dimensionality is usually much lower than the dimensionality that is given by the standard Karhunen-LoÃ¨ve technique. An analysis that demonstrates the feasability of this approach is presented.},
  date      = {Feb. 1971},
  doi       = {10.1109/T-C.1971.223208},
  file      = {:Fukunaga1971 - An Algorithm for Finding Intrinsic Dimensionality of Data.html:URL},
  issue     = {2},
  keywords  = {Data reduction, dimensionality reduction, interactive systems, intrinsic dimensionality, Karhunen-LoÃ¨ve expansion, multivariant data analysis, principal component, stochastic processes.},
  publisher = {IEEE},
}
@inproceedings{10.1145/2783258.2783405,
author = {Amsaleg, Laurent and Chelly, Oussama and Furon, Teddy and Girard, St\'{e}phane and Houle, Michael E. and Kawarabayashi, Ken-ichi and Nett, Michael},
title = {Estimating Local Intrinsic Dimensionality},
year = {2015},
isbn = {9781450336642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2783258.2783405},
doi = {10.1145/2783258.2783405},
abstract = {This paper is concerned with the estimation of a local measure of intrinsic dimensionality
(ID) recently proposed by Houle. The local model can be regarded as an extension of
Karger and Ruhl's expansion dimension to a statistical setting in which the distribution
of distances to a query point is modeled in terms of a continuous random variable.
This form of intrinsic dimensionality can be particularly useful in search, classification,
outlier detection, and other contexts in machine learning, databases, and data mining,
as it has been shown to be equivalent to a measure of the discriminative power of
similarity functions. Several estimators of local ID are proposed and analyzed based
on extreme value theory, using maximum likelihood estimation (MLE), the method of
moments (MoM), probability weighted moments (PWM), and regularly varying functions
(RV). An experimental evaluation is also provided, using both real and artificial
data.},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {29–38},
numpages = {10},
keywords = {indiscriminability, manifold learning, intrinsic dimension},
location = {Sydney, NSW, Australia},
series = {KDD '15}
}

@Book{P10,
  author    = {Pfl{\"{u}}ger, Dirk},
  publisher = {Verlag Dr. Hut},
  title     = {Spatially Adaptive Sparse Grids for High-Dimensional Problems},
  year      = {2010},
  address   = {M{\"{u}}nchen},
  isbn      = {9783868535556},
  month     = aug,
  school    = {Institut f{\"{u}}r Informatik, Technische Universit{\"{a}}t M{\"{u}}nchen},
  url       = {http://www5.in.tum.de/pub/pflueger10spatially.pdf},
}
@PhDThesis{F10,
  author =	 "Christian Feuersänger",
  title =	 "Sparse Grid Methods for Higher Dimensional Approximation",
  school =	 "Mathematisch–Naturwissenschaftliche Fakultät der der Rheinischen Friedrich–Wilhelms–Universität Bonn",
  year =	 "2010"
}@MasterThesis{V14,
  author =	 "Julian Valentin",
  title =	 "Hierarchische Optimierung mit Gradientenverfahren auf Dünngitterfunktionen",
  school =	 "Universität Stuttgart, Institut für Parallele und Verteilte Systeme",
  year =	 "2014",
  url =      {ftp://ftp.informatik.uni-stuttgart.de/pub/library/medoc.ustuttgart_fi/MSTR-3629/MSTR-3629.pdf}
}@INPROCEEDINGS{G13,
    author = {Jochen Garcke},
    title = {Sparse grids in a nutshell},
    booktitle = {Sparse grids and applications},
    year = {2013},
    pages = {57--80},
    publisher = {Springer}
}@inproceedings{P12,
    address = {Berlin Heidelberg},
    author = {Pfl{\"{u}}ger, Dirk},
    booktitle = {Sparse Grids and Applications},
    editor = {Garcke, Jochen and Griebel, Michael},
    month = oct,
    pages = {243--262},
    publisher = {Springer},
    series = {Lecture Notes in Computational Science and Engineering},
    title = {Spatially Adaptive Refinement},
    year = {2012},
    URL = {http://www5.in.tum.de/pub/pflueger12spatially_preprint.pdf}
}@article{S01,
 author = {Sobol\'{a}, I. M.},
 title = {Global Sensitivity Indices for Nonlinear Mathematical Models and Their Monte Carlo Estimates},
 journal = {Math. Comput. Simul.},
 issue_date = {Feb. 15, 2001},
 volume = {55},
 number = {1-3},
 month = feb,
 year = {2001},
 issn = {0378-4754},
 pages = {271--280},
 numpages = {10},
 url = {http://dx.doi.org/10.1016/S0378-4754(00)00270-6},
 doi = {10.1016/S0378-4754(00)00270-6},
 acmid = {373383},
 publisher = {Elsevier Science Publishers B. V.},
 address = {Amsterdam, The Netherlands, The Netherlands},
 keywords = {Monte Carlo method, mathematical modelling, quasi-Monte Carlo method, sensitivity analysis},
} 
@Article{Valentin2019,
  author        = {Julian Valentin},
  title         = {B-Splines for Sparse Grids: Algorithms and Application to Higher-Dimensional Optimization},
  year          = {2019},
  month         = oct,
  abstract      = {In simulation technology, computationally expensive objective functions are often replaced by cheap surrogates, which can be obtained by interpolation. Full grid interpolation methods suffer from the so-called curse of dimensionality, rendering them infeasible if the parameter domain of the function is higher-dimensional (four or more parameters). Sparse grids constitute a discretization method that drastically eases the curse, while the approximation quality deteriorates only insignificantly. However, conventional basis functions such as piecewise linear functions are not smooth (continuously differentiable). Hence, these basis functions are unsuitable for applications in which gradients are required. One example for such an application is gradient-based optimization, in which the availability of gradients greatly improves the speed of convergence and the accuracy of the results. This thesis demonstrates that hierarchical B-splines on sparse grids are well-suited for obtaining smooth interpolants for higher dimensionalities. The thesis is organized in two main parts: In the first part, we derive new B-spline bases on sparse grids and study their implications on theory and algorithms. In the second part, we consider three real-world applications in optimization: topology optimization, biomechanical continuum-mechanics, and dynamic portfolio choice models in finance. The results reveal that the optimization problems of these applications can be solved accurately and efficiently with hierarchical B-splines on sparse grids.},
  archiveprefix = {arXiv},
  doi           = {10.18419/opus-10504},
  eprint        = {1910.05379},
  file          = {:Valentin2019 - B Splines for Sparse Grids_ Algorithms and Application to Higher Dimensional Optimization.pdf:PDF},
  keywords      = {math.NA, cs.NA, G.1.1; G.1.6; G.4},
  primaryclass  = {math.NA},
}
@article{WANG2008366,
title = {Low discrepancy sequences in high dimensions: How well are their projections distributed?},
journal = {Journal of Computational and Applied Mathematics},
volume = {213},
number = {2},
pages = {366-386},
year = {2008},
issn = {0377-0427},
doi = {https://doi.org/10.1016/j.cam.2007.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0377042707000374},
author = {Xiaoqun Wang and Ian H. Sloan},
keywords = {Quasi-Monte Carlo methods, Low discrepancy sequences, Discrepancy, Multivariate integration},
abstract = {Quasi-Monte Carlo (QMC) methods have been successfully used to compute high-dimensional integrals arising in many applications, especially in finance. To understand the success and the potential limitation of QMC, this paper focuses on quality measures of point sets in high dimensions. We introduce the order-ℓ, superposition and truncation discrepancies, which measure the quality of selected projections of a point set on lower-dimensional spaces. These measures are more informative than the classical ones. We study their relationships with the integration errors and study the tractability issues. We present efficient algorithms to compute these discrepancies and perform computational investigations to compare the performance of the Sobol’ nets with that of the sets of Latin hypercube sampling and random points. Numerical results show that in high dimensions the superiority of the Sobol’ nets mainly derives from the one-dimensional projections and the projections associated with the earlier dimensions; for order-2 and higher-order projections all these point sets have similar behavior (on the average). In weighted cases with fast decaying weights, the Sobol’ nets have a better performance than the other two point sets. The investigation enables us to better understand the properties of QMC and throws new light on when and why QMC can have a better (or no better) performance than Monte Carlo for multivariate integration in high dimensions.}
}

@Article{WEI,
  author  = {Eric W. Weisstein},
  journal = {MathWorld--A Wolfram Web Resource},
  title   = {Hypercube Line Picking.},
  url     = {https://mathworld.wolfram.com/HypercubeLinePicking.html},
}
@article{BAILEY2007196,
title = {Box integrals},
journal = {Journal of Computational and Applied Mathematics},
volume = {206},
number = {1},
pages = {196-208},
year = {2007},
issn = {0377-0427},
doi = {https://doi.org/10.1016/j.cam.2006.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0377042706004250},
author = {D.H. Bailey and J.M. Borwein and R.E. Crandall},
keywords = {Box integrals, Multi-dimensional integrals, High-precision quadrature},
abstract = {By a “box integral” we mean here an expectation 〈|r⇒-q⇒|s〉 where r⇒ runs over the unit n-cube, with q⇒ and s fixed, explicitly:∫01⋯∫01((r1-q1)2+⋯+(rn-qn)2)s/2dr1…drn.The study of box integrals leads one naturally into several disparate fields of analysis. While previous studies have focused upon symbolic evaluation and asymptotic analysis of special cases (notably s=1), we work herein more generally—in interdisciplinary fashion—developing results such as: (1) analytic continuation (in complex s), (2) relevant combinatorial identities, (3) rapidly converging series, (4) statistical inferences, (5) connections to mathematical physics, and (6) extreme-precision quadrature techniques appropriate for these integrals. These intuitions and results open up avenues of experimental mathematics, with a view to new conjectures and theorems on integrals of this type.}
}
@article{huber1985projection,
  title={Projection pursuit},
  author={Huber, Peter J},
  journal={The annals of Statistics},
  pages={435--475},
  year={1985},
  publisher={JSTOR}
}

@Comment{jabref-meta: databaseType:bibtex;}
